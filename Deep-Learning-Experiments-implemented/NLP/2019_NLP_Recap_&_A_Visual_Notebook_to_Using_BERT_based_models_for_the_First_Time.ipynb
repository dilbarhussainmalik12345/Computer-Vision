{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019 NLP Recap & A Visual Notebook to Using BERT based models for the First Time.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/Colab_fastai/blob/master/NLP/2019_NLP_Recap_%26_A_Visual_Notebook_to_Using_BERT_based_models_for_the_First_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT94dKjU5HGN",
        "colab_type": "text"
      },
      "source": [
        "##2019: The Year of NLP's Imagenet Moment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXJ2ggezEGKK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<img src=https://raw.githubusercontent.com/nslatysheva/BERT_papers/master/BERT.gif width=\"1200\">\n",
        "\n",
        "                    Plot based on 169 BERT-related papers.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPLb5KyAD7x_",
        "colab_type": "text"
      },
      "source": [
        "*   **2019 has been a landmark year for NLP**, with new records across a variety of important tasks, from reading comprehension to sentiment analysis.\n",
        "\n",
        "*   **The key research trend that stands out is the rise of transfer learning in NLP**, which refers to using massive pre-trained models and fine-tuning them to your specific language-related task.\n",
        "\n",
        "*   **Transfer learning** allows you to reuse knowledge from previously built models, which **can give you a boost in performance and generalisation, while demanding much less labelled training data**.\n",
        "\n",
        "*   **The idea of pre-training models followed by task-specific fine-tuning is in itself not new** — computer vision practitioners regularly use models pre-trained on large datasets like ImageNet, and in NLP we have been doing “shallow” transfer learning for years by reusing word embeddings.\n",
        "\n",
        "*   But in 2019, with models like BERT, we saw a major shift towards deeper knowledge transfer by transferring entire models to new tasks — **essentially using large pre-trained language models as reusable language comprehension feature extractors**.\n",
        "*   **The emerging paradigm is**: why constantly learn language syntax and semantics from scratch for every new NLP task when you can reuse BERT’s solid grasp of language?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VAvTYT-EHtW",
        "colab_type": "text"
      },
      "source": [
        "**TimeLine for some Major NLP Projects before BERT**\n",
        "\n",
        "![Major NLP Projects Before BERT](https://miro.medium.com/max/2148/1*cozibGuv9jX8bheqyirEvA.png)\n",
        "\n",
        "**TimeLines for projects after BERT**\n",
        "![TimeLines for projects after BERT](https://miro.medium.com/max/4096/1*ZUHC1z5V6AsR_MOltzW-NQ.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK7UknmQ96H1",
        "colab_type": "text"
      },
      "source": [
        "Some people in the community question the relevance of keeping on training larger and larger Transformer especially when you take into account the financial and environmental cost of training. **Here’s are some of the latest large models and their size in millions of parameters**.\n",
        "\n",
        "<img src=https://miro.medium.com/max/4140/1*IFVX74cEe8U5D1GveL1uZA.png width=\"1200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfUpVXHYFMYS",
        "colab_type": "text"
      },
      "source": [
        "Okay so we have so many new models and research going on but how are we doing versus human baselines??\n",
        "\n",
        "***Some Benchmark Scores***\n",
        "\n",
        "1. **Glue Benchmark**\n",
        "![Glue Benchmarks as of Jan 2nd 2020](https://miro.medium.com/max/3076/1*8WjKoA1jPzLNQGisEo0toQ.png)\n",
        "\n",
        "**Glue Benchmark** leaderboard as of 2-Jan-2020 [source](https://gluebenchmark.com/leaderboard).\n",
        "\n",
        "\n",
        "---\n",
        "2. **SQuAD 2.0 Leaderboard**\n",
        "![SQuAD2.0 benchmark scores](https://miro.medium.com/max/1646/1*tbie_0WG7jmNB0X8SVV8aA.png)\n",
        "\n",
        "**SQuAD 2.0 Leaderboard** as of 2-Jan-2020 — [Source](https://rajpurkar.github.io/SQuAD-explorer/)\n",
        "\n",
        "---\n",
        "\n",
        "3. **SuperGlue Benchmark**\n",
        "\n",
        "There is now a **SuperGlue benchmark** also which consists of more difficult language understanding tasks .\n",
        "\n",
        "![SuperGlue Benchmark](https://miro.medium.com/max/3086/1*hSkFJLEOD2mMzKskARvu_g.png)\n",
        "**SuperGlue Benchmark** leaderboard as of 2-Jan-2020 — [Source](https://super.gluebenchmark.com/leaderboard).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjeOcPE7-MLu",
        "colab_type": "text"
      },
      "source": [
        "Now that’s a lot of BERT papers. Some notes on this plot:\n",
        "\n",
        "*   It’s interesting to observe the (fairly short) lag between the publication of the original paper in November 2018, and the **flood of papers starting around January 2019**.\n",
        "\n",
        "*   The initial wave of BERT papers tended to focus on **immediate extensions and applications** of the core BERT model (red, purple and orange dots), like adapting BERT for recommendations systems, sentiment analysis, text summarisation, and document retrieval.\n",
        "\n",
        "\n",
        "*   Then, starting in April, a collection of papers probing the internal mechanisms of BERT were published (in green), like **understanding how BERT models hierarchical linguistic phenomena and analysing the redundancy between attention heads**. Of particular interest is the paper “BERT Rediscovers the Classical NLP Pipeline”, in which the authors find that BERT’s internal computations mirror the traditional NLP workflow (first do parts-of-speech tagging, then dependency parsing, then entity tagging, etc.).\n",
        "\n",
        "*   Around September, a collection of **papers focused on compressing the model size of BERT were released (cyan), like the DistilBERT, ALBERT and TinyBERT papers**. For instance, DistilBERT model from HuggingFace is a compressed version of BERT with half the number of parameters (from 110 million down to 66 million) but 95% of the performance on important NLP tasks (see the GLUE benchmarks). The original BERT models are not exactly lightweight, and this is a problem in places where computational resources are not plentiful (like mobile phones).\n",
        "\n",
        "*   This list of BERT papers is very likely to be incomplete. I wouldn’t be surprised if the true number of very BERT-relevant papers is double my figure. As a rough upper bound, **the number of papers that cite the original BERT paper is currently over 3100.**\n",
        "\n",
        "*   Main use case of BERT **Google Search now uses BERT** in their Search Engine as well Real world impact.\n",
        "Going from keyword based search to Natural Language Query understanding.\n",
        "\n",
        "* **GPT-2 Impact Release of this model was the main event as success of AI let alone NLP by generating almost real world like articles.** \n",
        "\n",
        "* Deemed Dangerous and delayed release to clarity first on its misuse. Coherent Natural Language Generation. Logically sounding Paragraphs and not just sentences.\n",
        "*    From ‘F’ to ‘A’ on the N.Y. Regents Science Exams: An Overview of the Aristo Project [BERT and Roberta on Class 8 Science exams](https://arxiv.org/pdf/1909.01958.pdf). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0odVFgy9rBx",
        "colab_type": "text"
      },
      "source": [
        "**How is BERT trained?**\n",
        "\n",
        "But what tasks is BERT trained on that encourage it to learn such a good, generally useful understanding of language? Future work tweaked the learning strategy, but the original paper used two tasks:\n",
        "\n",
        "1.   **The Masked Language Model (MLM) task**. This task encouraged the model to learn good representations at the word-level and at the sentence-level (since a sentence is the totality of word representations). Briefly, **15% of the words in a sentence are randomly chosen and hidden (or “masked”) with a {MASK} token. The model’s job is to predict the identity of these hidden words, making use of both the words before and after the {MASK}** — hence, we are trying to reconstruct the text from a corrupted input, and both left and right contexts are used to make predictions. This allows us to build up representations of words that take all of the context into account. BERT learns its bidirectional representations simultaneously, in contrast to methods like **ELMo (an RNN-based language model used for generating context-aware word embeddings), where left-to-right and right-to-left representations are independently learned by two language models and then concatenated**. We could say that **ELMo is a ‘shallow bidirectional’ model whereas BERT is a ‘deep bidirectional’ model**.\n",
        "\n",
        "\n",
        "2.   **The Next Sentence Prediction (NSP) task**. If our model is going to be used as the basis for language understanding, it would be good for it to have some knowledge of inter-sentence coherence. To encourage the model to learn about the relationship between sentences, we add the Next Sentence Prediction task, in which the model has to predict if a pair of sentences are related, namely if one is likely to come after another. Positive training pairs are real adjacent sentences in the corpus; negative training pairs are randomly sampled from the corpus. It’s not a perfect system, since randomly sampled pairs could actually be related, but it is good enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJmnkcbk9NP8",
        "colab_type": "text"
      },
      "source": [
        "##Word Embeddings Refresher "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWxKswCPCVSr",
        "colab_type": "text"
      },
      "source": [
        "This is a word embedding for the word “king” (GloVe vector trained on Wikipedia):\n",
        "\n",
        "[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n",
        "\n",
        "It’s a list of 50 numbers. We can’t tell much by looking at the values. But let’s visualize it a bit so we can compare it other word vectors. Let’s put all these numbers in one row:\n",
        "![Vector for word King](https://jalammar.github.io/images/word2vec/king-white-embedding.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcSro6qZSMQz",
        "colab_type": "text"
      },
      "source": [
        "Let’s color code the cells based on their values (red if they’re close to 2, white if they’re close to 0, blue if they’re close to -2):\n",
        "![color coded vector for King](https://jalammar.github.io/images/word2vec/king-colored-embedding.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEPnpigoTFh-",
        "colab_type": "text"
      },
      "source": [
        "Presenting different color coded words\n",
        "\n",
        "![different words](https://jalammar.github.io/images/word2vec/queen-woman-girl-embeddings.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDMK0EEqSehS",
        "colab_type": "text"
      },
      "source": [
        "The famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:\n",
        "![finding most similar](https://jalammar.github.io/images/word2vec/king-man+woman-gensim.png)\n",
        "\n",
        "Using the Gensim library in python, we can add and subtract word vectors, and it would find the most similar words to the resulting vector. The image shows a list of the most similar words, each with its cosine similarity.\n",
        "\n",
        "We can visualize this analogy as we did previously:\n",
        "\n",
        "![show the color representation](https://jalammar.github.io/images/word2vec/king-analogy-viz.png).\n",
        "\n",
        "The resulting vector from \"king-man+woman\" doesn't exactly equal \"queen\", but \"queen\" is the closest word to it from the 400,000 word embeddings we have in this collection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izA3-6kffbdT",
        "colab_type": "text"
      },
      "source": [
        "# A Visual Notebook to Using BERT for the First Time.ipynb\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification.png\" />\n",
        "\n",
        "In this notebook, we will use pre-trained deep learning model to process some text. We will then use the output of that model to classify the text. The text is a list of sentences from film reviews. And we will calssify each sentence as either speaking \"positively\" about its subject of \"negatively\".\n",
        "\n",
        "## Models: Sentence Sentiment Classification\n",
        "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
        "\n",
        "Under the hood, the model is actually made up of two model.\n",
        "\n",
        "* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
        "* The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the sentence as either positive or negative (1 or 0, respectively).\n",
        "\n",
        "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\n",
        "\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />\n",
        "\n",
        "## Dataset\n",
        "The dataset we will use in this example is [SST2](https://nlp.stanford.edu/sentiment/index.html), which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):\n",
        "\n",
        "\n",
        "<table class=\"features-table\">\n",
        "  <tr>\n",
        "    <th class=\"mdc-text-light-green-600\">\n",
        "    sentence\n",
        "    </th>\n",
        "    <th class=\"mdc-text-purple-600\">\n",
        "    label\n",
        "    </th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      1\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      apparently reassembled from the cutting room floor of any given daytime soap\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      0\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      they presume their audience won't sit still for a sociology lesson\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      0\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      1\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      1\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "## Installing the transformers library\n",
        "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rk8QmnnzNOB",
        "colab_type": "text"
      },
      "source": [
        "# Notebook Overview\n",
        "\n",
        "So here’s the game plan with this tutorial. We will first use the trained distilBERT to generate sentence embeddings for 2,000 sentences.\n",
        "\n",
        "\n",
        "\n",
        "![Step #1 Use DistilBERT to embed all sentences](https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png)\n",
        "\n",
        "We will not touch distilBERT after this step. It’s all Scikit Learn from here. We do the usual train/test split on this dataset:\n",
        "\n",
        "![Step #2 Train-Test Split for Model#2](https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png)\n",
        "\n",
        "Then we train the logistic regression model on the training set:\n",
        "\n",
        "![Step #3 Train Logistic Regression Model using Training Set](https://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To9ENLU90WGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvFvBLJV0Dkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ-42fh0hjsF",
        "colab_type": "text"
      },
      "source": [
        "## Importing the dataset\n",
        "We'll use pandas to read the dataset and load it into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyoj29J24hPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMVE3waNhuNj",
        "colab_type": "text"
      },
      "source": [
        "For performance reasons, we'll only use 2,000 sentences from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTM3hOHW4hUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_1 = df[:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRc2L89hh1Tf",
        "colab_type": "text"
      },
      "source": [
        "We can ask pandas how many sentences are labeled as \"positive\" (value 1) and how many are labeled \"negative\" (having the value 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvcfcCP5xpZ",
        "colab_type": "code",
        "outputId": "3259900c-9e43-4277-fc7f-4d72b27d0d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "batch_1[1].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1041\n",
              "0     959\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_MO08_KiAOb",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Pre-trained BERT model\n",
        "Let's now load a pre-trained BERT model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1InADgf5xm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "'''\n",
        "# Transformers has a unified API\n",
        "# for 10 transformer architectures and 30 pretrained weights.\n",
        "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
        "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n",
        "          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n",
        "          (GPT2Model,       GPT2Tokenizer,       'gpt2'),\n",
        "          (CTRLModel,       CTRLTokenizer,       'ctrl'),\n",
        "          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
        "          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),\n",
        "          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
        "          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),\n",
        "          (RobertaModel,    RobertaTokenizer,    'roberta-base'),\n",
        "          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
        "         ]\n",
        "\n",
        "# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`\n",
        "\n",
        "# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n",
        "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
        "                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n",
        "\n",
        "# All the classes for an architecture can be initiated from pretrained weights for this architecture\n",
        "# Note that additional weights added for fine-tuning are only initialized\n",
        "'''\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZDBMn3wiSX6",
        "colab_type": "text"
      },
      "source": [
        "Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n",
        "\n",
        "## Model #1: Preparing the Dataset\n",
        "Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n",
        "\n",
        "### Tokenization\n",
        "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg82ndBA5xlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwjUwYgi-uL",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n",
        "\n",
        "### Padding\n",
        "After tokenization, `tokenized` is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size (66 in our case), so we can represent the input as one 2-d array, rather than a list of lists (of different lengths).\n",
        "\n",
        "<img src=https://jalammar.github.io/images/distilBERT/bert-input-tensor.png width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URn-DWJt5xhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "max_len = max(max_len,66) # to make it more understandable as our iamges have tokens of length 66\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdjg306wjjmL",
        "colab_type": "text"
      },
      "source": [
        "Our dataset is now in the `padded` variable, we can view its dimensions below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdi7uXo95xeq",
        "colab_type": "code",
        "outputId": "712848db-1b81-4ee5-c32c-61852f55d218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(padded).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 66)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDZBsYSDjzDV",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_iGRNa_Ozc",
        "colab_type": "code",
        "outputId": "048bdc42-9cd0-4e5a-bcdb-752465188c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 66)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-CQB9-kN99",
        "colab_type": "text"
      },
      "source": [
        "## Model #1: And Now, Deep Learning!\n",
        "Now that we have our model and inputs ready, let's run our model!\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-model-input-output-1.png\" />\n",
        "\n",
        "The `model()` function runs our sentences through BERT. The results of the processing will be returned into `last_hidden_states`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39UVjAV56PJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoCep_WVuB3v",
        "colab_type": "text"
      },
      "source": [
        "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
        "\n",
        "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9t60At16PVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZVU66Gurr-",
        "colab_type": "text"
      },
      "source": [
        "The labels indicating which sentence is positive and negative now go into the `labels` variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD3fX2yh6PTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = batch_1[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaoEvM2evRx1",
        "colab_type": "text"
      },
      "source": [
        "## Model #2: Train/Test Split\n",
        "Let's now split our datset into a training set and testing set (even though we're using 2,000 sentences from the SST2 training set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddAqbkoU6PP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9bhSJpcv1Bl",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n",
        "\n",
        "### [Bonus] Grid Search for Parameters\n",
        "We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyEwr7yYD3Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
        "# grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
        "# grid_search.fit(train_features, train_labels)\n",
        "\n",
        "# print('best parameters: ', grid_search.best_params_)\n",
        "# print('best scrores: ', grid_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCT9u8vAwnID",
        "colab_type": "text"
      },
      "source": [
        "We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. `LogisticRegression(C=5.2)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-EVWx4CzBc",
        "colab_type": "code",
        "outputId": "95bbb26c-8537-41e9-88c1-ba717fe74eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rUMKuVgwzkY",
        "colab_type": "text"
      },
      "source": [
        "##Recap Sentence Journey\n",
        "\n",
        "![sentence journey](https://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png)\n",
        "##Slicing the important part\n",
        "For sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.\n",
        "![alt text](https://jalammar.github.io/images/distilBERT/bert-output-cls-senteence-embeddings.png)\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/logistic-regression-dataset-features-labels.png\" />\n",
        "\n",
        "## Evaluating Model #2\n",
        "So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:\n",
        "\n",
        "![Train Test Split](https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCoyxRJ7ECTA",
        "colab_type": "code",
        "outputId": "b9d7f515-7ef4-44b5-b10a-816bd7909e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75oyhr3VxHoE",
        "colab_type": "text"
      },
      "source": [
        "How good is this score? What can we compare it against? Let's first look at a dummy classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnwgmqNG7i5l",
        "colab_type": "code",
        "outputId": "1de9450d-45b8-44cc-e85e-dc5deb5be80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dummy classifier score: 0.506 (+/- 0.05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRGHt2TvCOfh",
        "colab_type": "code",
        "outputId": "5a42480e-8780-4ecd-dc1a-8fdc56711850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Import Random Forest Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "rf_clf_def = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "rf_clf_def.fit(train_features, train_labels)\n",
        "\n",
        "scores = cross_val_score(rf_clf_def, train_features, train_labels)\n",
        "print(\"Default RF classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default RF classifier score: 0.795 (+/- 0.04)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lg4LOpoxSOR",
        "colab_type": "text"
      },
      "source": [
        "So our model clearly does better than a dummy classifier. But how does it compare against the best models?\n",
        "\n",
        "## Proper SST2 scores\n",
        "For reference, the [highest accuracy score](http://nlpprogress.com/english/sentiment_analysis.html) for this dataset is currently **96.8**. DistilBERT can be trained to improve its score on this task – a process called **fine-tuning** which updates BERT’s weights to make it achieve a better performance in this sentence classification task (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of **90.7**. The full size BERT model achieves **94.9**.\n",
        "\n",
        "\n",
        "\n",
        "And that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at [fine-tuning](https://huggingface.co/transformers/examples.html#glue). You can also go back and switch from distilBERT to BERT and see how that works.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ta_20NSAz3c",
        "colab_type": "text"
      },
      "source": [
        "Credits:-\n",
        "Based on this [article](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/) by Jay Alammar.\n",
        "\n",
        "[2019 NLP Recap article](https://towardsdatascience.com/2019-the-year-of-bert-354e8106f7ba) by [Natasha](https://towardsdatascience.com/@natasha.latysheva).\n",
        "\n",
        "The Original [Colab Notebook](https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb#scrollTo=q1InADgf5xm2) is here. \n",
        "\n",
        "Some Other links to explore:-\n",
        "\n",
        "https://towardsdatascience.com/2019-the-year-of-bert-354e8106f7ba\n",
        "\n",
        "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
        "\n",
        "https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\n",
        "\n",
        "https://towardsdatascience.com/2019-year-of-bert-and-transformer-f200b53d05b9\n",
        "\n",
        "https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html\n",
        "\n",
        "**The Libraries that I prefer** :-\n",
        "\n",
        "[Flair Tutorials](https://github.com/flairNLP/flair/tree/master/resources/docs).\n",
        "\n",
        "[Huggingface Github Repo](https://github.com/huggingface/transformers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4V0nBpRSqaA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Who am I ??**\n",
        "\n",
        "Navneet Kumar Chaudhary\n",
        "\n",
        "Data Scientist working on Embedding based Recommender Systems @ [OLXPEOPLE](https://www.olxpeople.com/) / @ [AASAANJOBS](https://www.aasaanjobs.com/)\n",
        "\n",
        "**Find me** \n",
        "\n",
        "@ [LinkedIn](https://www.linkedin.com/in/navneetkrc/)\n",
        "\n",
        "@ [Github](https://github.com/navneetkrc)\n",
        "\n",
        "Find some more projects to explore and create your own models from [my Github Repo](https://github.com/navneetkrc/Colab_fastai). \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}