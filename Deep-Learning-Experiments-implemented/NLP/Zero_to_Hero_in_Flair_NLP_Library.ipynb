{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zero to Hero in Flair NLP Library.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/Colab_fastai/blob/master/Zero_to_Hero_in_Flair_NLP_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CQpqE1VyNHlt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "rVbCDs6wMdk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
        "!pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Nj7O9_2sumr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnfZaO83NB50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#1. NLP Base Types and tasks"
      ]
    },
    {
      "metadata": {
        "id": "PT__1hBNNlzt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Check for some basics tasks"
      ]
    },
    {
      "metadata": {
        "id": "tRNSgtNbNTPe",
        "colab_type": "code",
        "outputId": "98452f7d-030f-4b45-fc17-af3b0a830d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# make a sentence\n",
        "sentence = Sentence('I love Amsterdam .')\n",
        "\n",
        "# load the NER tagger\n",
        "tagger = SequenceTagger.load('ner')\n",
        "\n",
        "# run NER over sentence\n",
        "tagger.predict(sentence)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"I love Amsterdam .\" - 4 Tokens]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "RCosXqIXNkFk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Done! The Sentence now has entity annotations. Print the sentence to see what the tagger found.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2yq-9aAqNs6Z",
        "colab_type": "code",
        "outputId": "1b2d466d-17e6-4ac0-e902-73ee5aaac99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(sentence)\n",
        "print('The following NER tags are found:')\n",
        "\n",
        "# iterate over entities and print\n",
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"I love Amsterdam .\" - 4 Tokens\n",
            "The following NER tags are found:\n",
            "LOC-span [3]: \"Amsterdam\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mBBfJAsstiqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UCI0qLKStlNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Basic1- Creating a Sentence"
      ]
    },
    {
      "metadata": {
        "id": "X4GnV6DRNx4F",
        "colab_type": "code",
        "outputId": "adf690b1-7798-41af-b77e-edc1e52081d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The sentence objects holds a sentence that we may want to embed or tag\n",
        "from flair.data import Sentence\n",
        "\n",
        "# Make a sentence object by passing a whitespace tokenized string\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# Print the object to see what's in there\n",
        "print(sentence)\n",
        "\n",
        "#expected output-> Sentence: \"The grass is green .\" - 5 Tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"The grass is green .\" - 5 Tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eJ4Nm174txiC",
        "colab_type": "code",
        "outputId": "a80b9162-cd6d-4d33-e36a-cd597c4a9d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "##The print-out tells us that the sentence consists of 5 tokens. You can access the tokens of a sentence via their token id or with their index:\n",
        "\n",
        "# using the token id\n",
        "print(sentence.get_token(4))\n",
        "\n",
        "# using the index itself\n",
        "print(sentence[3])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: 4 green\n",
            "Token: 4 green\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kjOzaUrTuh3B",
        "colab_type": "code",
        "outputId": "8afee9ea-e253-4ce8-9b71-3e0c1c549f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "for token in sentence:\n",
        "    print(token)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: 1 The\n",
            "Token: 2 grass\n",
            "Token: 3 is\n",
            "Token: 4 green\n",
            "Token: 5 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oOG7eJmlvNBZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "6mH4Oyw7vUkM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In some use cases, you might not have your text already tokenized. For this case, we added a simple tokenizer using the lightweight segtok library.\n",
        "\n",
        "Simply use the use_tokenizer flag when instantiating your Sentence with an untokenized string:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hM31xEUvvO2n",
        "colab_type": "code",
        "outputId": "620d444d-6a00-421f-ae85-a1bb1c9d7947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "# Make a sentence object by passing an untokenized string and the 'use_tokenizer' flag\n",
        "sentence = Sentence('The grass is green.', use_tokenizer=True)\n",
        "\n",
        "# Print the object to see what's in there\n",
        "print(sentence)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"The grass is green .\" - 5 Tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6QnosUJnvjMX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Adding Tags to tokens"
      ]
    },
    {
      "metadata": {
        "id": "MV8czKK5vYw6",
        "colab_type": "code",
        "outputId": "c7aa4095-a841-4002-f367-9b5523282c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# add a tag to a word in the sentence\n",
        "sentence[3].add_tag('ner', 'color')\n",
        "\n",
        "# print the sentence with all tags of this type\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The grass is green <color> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HgxsseWUvpwf",
        "colab_type": "code",
        "outputId": "c37e030e-1f93-4619-ae95-872589be2d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from flair.data import Label\n",
        "\n",
        "tag: Label = sentence[3].get_tag('ner')\n",
        "\n",
        "print(f'\"{sentence[3]}\" is tagged as \"{tag.value}\" with confidence score \"{tag.score}\"')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Token: 4 green\" is tagged as \"color\" with confidence score \"1.0\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1aUW3t8pwEQj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should print:\n",
        "\n",
        "\"Token: 4 green\" is tagged as \"color\" with confidence score \"1.0\"\n",
        "\n",
        "Also our color tag has a score of 1.0 since we manually added it. If a tag is predicted by our sequence labeler, the score value will indicate classifier confidence.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bN-AcmmhwQny",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Adding Labels to Sentences"
      ]
    },
    {
      "metadata": {
        "id": "wdor7-v0wXGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A Sentence can have one or multiple labels that can for example be used in text classification tasks. For instance, the example below shows how we add the label 'sports' to a sentence, thereby labeling it as belonging to the sports category."
      ]
    },
    {
      "metadata": {
        "id": "X7ic6mu3v-rS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = Sentence('France is the current world cup winner.')\n",
        "\n",
        "# add a label to a sentence\n",
        "sentence.add_label('sports')\n",
        "\n",
        "# a sentence can also belong to multiple classes\n",
        "sentence.add_labels(['sports', 'world cup'])\n",
        "\n",
        "# you can also set the labels while initializing the sentence\n",
        "sentence = Sentence('France is the current world cup winner.', labels=['sports', 'world cup'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_msAzDtRwfJp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Labels are also of the Label class. So, you can print a sentence's labels like this:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JR7ECzktwgRL",
        "colab_type": "code",
        "outputId": "266699ab-4014-4744-c535-0e7917d84d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "sentence = Sentence('France is the current world cup winner.', labels=['sports', 'world cup'])\n",
        "\n",
        "print(sentence)\n",
        "for label in sentence.labels:\n",
        "    print(label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"France is the current world cup winner.\" - 7 Tokens\n",
            "sports (1.0)\n",
            "world cup (1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N-nYr7Rrwpga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should print:\n",
        "\n",
        "sports (1.0)\n",
        "\n",
        "world cup (1.0)\n",
        "\n",
        "**This indicates that the sentence belongs to these two classes, each with confidence score 1.0.**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "i0pWs5aoyAyb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**In next column we will check about the use of pre-trained model for tagging our text in the next segment.**"
      ]
    },
    {
      "metadata": {
        "id": "W15ZynHSyKlq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2. Use of Pretrained model to tag your text"
      ]
    },
    {
      "metadata": {
        "id": "aFGl3XVhD153",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Tagging your Text\n",
        "Here, we show how to use our pre-trained models to tag your text data.\n",
        "\n",
        "Tagging with Pre-Trained Sequence Tagging Models\n",
        "Let's use a pre-trained model for named entity recognition (NER). \n",
        "\n",
        "This model was trained over the English CoNLL-03 task and can recognize 4 different entity types."
      ]
    },
    {
      "metadata": {
        "id": "OuqEEpBLD28t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger = SequenceTagger.load('ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8Oolc26EJhf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All you need to do is use the predict() method of the tagger on a sentence. \n",
        "\n",
        "This will add predicted tags to the tokens in the sentence. Lets use a sentence with two named entities:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HgV_hQ9AFAk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5ff47b2-1990-460a-b5cb-32232bfe725d"
      },
      "cell_type": "code",
      "source": [
        "sentence = Sentence('George Washington went to Washington .')\n",
        "\n",
        "# predict NER tags\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print sentence with predicted tags\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "George <B-PER> Washington <E-PER> went to Washington <S-LOC> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KTQpBWCRFHlf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should print:\n",
        "\n",
        "George <B-PER> Washington <E-PER> went to Washington <S-LOC> . \n"
      ]
    },
    {
      "metadata": {
        "id": "eAXhravDGIKy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Getting Annotated Spans\n",
        "Many sequence labeling methods annotate spans that consist of multiple words, such as \"George Washington\" in our example sentence. You can directly get such spans in a tagged sentence like this:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QlPhWjGqGSVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4391d02c-8e61-4d24-8531-cc1af23b44a0"
      },
      "cell_type": "code",
      "source": [
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PER-span [1,2]: \"George Washington\"\n",
            "LOC-span [5]: \"Washington\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MboeMjBRGVnv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should print:\n",
        "\n",
        "PER-span [1,2]: \"George Washington\"\n",
        "\n",
        "LOC-span [5]: \"Washington\"\n",
        "\n",
        "Which indicates that \"George Washington\" is a person (PER) and \"Washington\" is a location (LOC). Each such Span has a text, a tag value, its position in the sentence and \"score\" that indicates how confident the tagger is that the prediction is correct. You can also get additional information, such as the position offsets of each entity in the sentence by calling:\n"
      ]
    },
    {
      "metadata": {
        "id": "xyVwVDGBGqKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f997ae05-7d9c-4d49-f5db-d39bcfa499c3"
      },
      "cell_type": "code",
      "source": [
        "print(sentence.to_dict(tag_type='ner'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': 'George Washington went to Washington .', 'labels': [], 'entities': [{'text': 'George Washington', 'start_pos': 0, 'end_pos': 17, 'type': 'PER', 'confidence': 0.9999169111251831}, {'text': 'Washington', 'start_pos': 26, 'end_pos': 36, 'type': 'LOC', 'confidence': 0.9990814924240112}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8W2AkLhFG2Sa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This should print:\n",
        "\n",
        "{'text': 'George Washington went to Washington .',\n",
        "    'entities': [      \n",
        "        {'text': 'George Washington', 'start_pos': 0, 'end_pos': 17, 'type': 'PER', 'confidence': 0.999},        \n",
        "        {'text': 'Washington', 'start_pos': 26, 'end_pos': 36, 'type': 'LOC', 'confidence': 0.998}\n",
        "    ]}"
      ]
    },
    {
      "metadata": {
        "id": "uy4vD58PJm3u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Tagging Multilingual Text\n",
        "If you have text in many languages (such as English and German), you can use our new multilingual models:\n",
        "\n",
        "Same approach I will try to use the same for HINGLISH (Hindi+English) Dataset as well\n"
      ]
    },
    {
      "metadata": {
        "id": "wx6wq8pfFEe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8c8b0ec-fcac-4b44-a498-efb779cec3aa"
      },
      "cell_type": "code",
      "source": [
        "# load model\n",
        "tagger = SequenceTagger.load('pos-multi')\n",
        "\n",
        "# text with English and German sentences\n",
        "sentence = Sentence('George Washington went to Washington . Dort kaufte er einen Hut .')\n",
        "\n",
        "# predict PoS tags\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print sentence with predicted tags\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "George <PROPN> Washington <PROPN> went <VERB> to <ADP> Washington <PROPN> . <PUNCT> Dort <ADV> kaufte <VERB> er <PRON> einen <DET> Hut <NOUN> . <PUNCT>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ad8dDhdtKWOi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should print:\n",
        "\n",
        "George <PROPN> Washington <PROPN> went <VERB> to <ADP> Washington <PROPN> . <PUNCT>\n",
        "\n",
        "Dort <ADV> kaufte <VERB> er <PRON> einen <DET> Hut <NOUN> . <PUNCT>\n",
        "So, both 'went' and 'kaufte' are identified as VERBs in these sentences.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4MZblCZXbYiT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Check for HINGLISH(Hindi+English) multilingual text"
      ]
    },
    {
      "metadata": {
        "id": "YUJxxWQXbl7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d02a6db-125c-4899-f270-5d63c1de0577"
      },
      "cell_type": "code",
      "source": [
        "# load model\n",
        "tagger = SequenceTagger.load('pos-multi')\n",
        "\n",
        "# text with English and Hindi sentences\n",
        "sentence = Sentence('Humlog Delhi ja rahe hai . Hare rang ki ghaas')\n",
        "\n",
        "# predict PoS tags\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print sentence with predicted tags\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Humlog <PROPN> Delhi <PROPN> ja <CCONJ> rahe <NOUN> hai <NOUN> . <PUNCT> Hare <PROPN> rang <NOUN> ki <ADP> ghaas <NOUN>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ydVYuyg7K2vj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Experimental: Semantic Frame Detection\n",
        "For English, we provide a pre-trained model that detects semantic frames in text, trained using Propbank 3.0 frames. This provides a sort of word sense disambiguation for frame evoking words, and we are curious what researchers might do with this.\n",
        "\n",
        "Here's an example:\n"
      ]
    },
    {
      "metadata": {
        "id": "1LAyt6BOK__m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load model\n",
        "tagger = SequenceTagger.load('frame')\n",
        "\n",
        "# make German sentence\n",
        "sentence_1 = Sentence('George returned to Berlin to return his hat .')\n",
        "sentence_2 = Sentence('He had a look at different hats .')\n",
        "\n",
        "# predict NER tags\n",
        "tagger.predict(sentence_1)\n",
        "tagger.predict(sentence_2)\n",
        "\n",
        "# print sentence with predicted tags\n",
        "print(sentence_1.to_tagged_string())\n",
        "print(sentence_2.to_tagged_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJ8eg5xCLDSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should print:\n",
        "\n",
        "George returned <return.01> to Berlin to return <return.02> his hat .\n",
        "\n",
        "He had <have.LV> a look <look.01> at different hats .\n"
      ]
    },
    {
      "metadata": {
        "id": "sZSopQEmLgdZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "As we can see, the frame detector makes a distinction in sentence 1 between two different meanings of the word 'return'. 'return.01' means returning to a location, while 'return.02' means giving something back.\n",
        "\n",
        "Similarly, in sentence 2 the frame detector finds a light verb construction in which 'have' is the light verb and 'look' is a frame evoking word.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LQ6b3zlrLseG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Tagging a List of Sentences\n",
        "Often, you may want to tag an entire text corpus. In this case, you need to split the corpus into sentences and pass a list of Sentence objects to the .predict() method.\n",
        "\n",
        "For instance, you can use the sentence splitter of segtok to split your text:\n"
      ]
    },
    {
      "metadata": {
        "id": "-WJM2IigKVcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "291a77f1-1e0d-45d1-b9f5-d3365799ed3c"
      },
      "cell_type": "code",
      "source": [
        "# your text of many sentences\n",
        "text = \"This is a sentence. This is another sentence. I love Berlin.\"\n",
        "\n",
        "# use a library to split into sentences\n",
        "from segtok.segmenter import split_single\n",
        "sentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(text)]\n",
        "\n",
        "# predict tags for list of sentences\n",
        "tagger: SequenceTagger = SequenceTagger.load('ner')\n",
        "tagger.predict(sentences)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"This is a sentence .\" - 5 Tokens,\n",
              " Sentence: \"This is another sentence .\" - 5 Tokens,\n",
              " Sentence: \"I love Berlin .\" - 4 Tokens]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "4eK7GKu1L-g6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using the mini_batch_size parameter of the .predict() method, you can set the size of mini batches passed to the tagger. Depending on your resources, you might want to play around with this parameter to optimize speed.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "U468xUPVM3V_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Tagging with Pre-Trained Text Classification Models\n",
        "Let's use a pre-trained model for detecting positive or negative comments. This model was trained over the IMDB dataset and can recognize positive and negative sentiment in English text.\n"
      ]
    },
    {
      "metadata": {
        "id": "03ddv7XbKVlR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.models import TextClassifier\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "#All you need to do is use the predict() method of the classifier on a sentence.\n",
        "#This will add the predicted label to the sentence. Lets use a sentence with negative sentiment:\n",
        "\n",
        "sentence = Sentence('This film hurts. It is so bad that I am confused.')\n",
        "\n",
        "# predict NER tags\n",
        "classifier.predict(sentence)\n",
        "\n",
        "# print sentence with predicted labels\n",
        "print(sentence.labels)\n",
        "\n",
        "#This should print:\n",
        "#[NEGATIVE (1.0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETuvGZ8kNfq-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**In next column we will check about the use of word embeddings to embed our text**"
      ]
    },
    {
      "metadata": {
        "id": "rsX-n4ucybqR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3. Use of word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "3qTB5b1CIAXn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Embeddings\n",
        "All word embedding classes inherit from the TokenEmbeddings class and implement the embed() method which you need to call to embed your text. This means that for most users of Flair, the complexity of different embeddings remains hidden behind this interface. Simply instantiate the embedding class you require and call embed() to embed your text.\n",
        "\n",
        "All embeddings produced with our methods are Pytorch vectors, so they can be immediately used for training and fine-tuning."
      ]
    },
    {
      "metadata": {
        "id": "S1lOjJEzIMjl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Classic Word Embeddings\n",
        "Classic word embeddings are static and word-level, meaning that each distinct word gets exactly one pre-computed embedding. Most embeddings fall under this class, including the popular GloVe or Komnios embeddings.\n",
        "\n",
        "Simply instantiate the WordEmbeddings class and pass a string identifier of the embedding you wish to load. So, if you want to use GloVe embeddings, pass the string 'glove' to the constructor:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xtF9yU_oFeLR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "\n",
        "# init embedding\n",
        "glove_embedding = WordEmbeddings('glove')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JmeMYyLIy4-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, create an example sentence and call the embedding's embed() method. You can also pass a list of sentences to this method since some embedding types make use of batching to increase speed.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CGI7EoWiIysm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1326
        },
        "outputId": "f9e8759e-5a45-4c65-9981-63f0d5deef32"
      },
      "cell_type": "code",
      "source": [
        "# create sentence.\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# embed a sentence using glove.\n",
        "glove_embedding.embed(sentence)\n",
        "\n",
        "# now check out the embedded tokens.\n",
        "for token in sentence:\n",
        "    print(token)\n",
        "    print(token.embedding)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: 1 The\n",
            "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
            "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
            "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
            "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
            "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
            "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
            "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
            "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
            "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
            "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
            "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
            "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
            "        -0.5203, -0.1459,  0.8278,  0.2706])\n",
            "Token: 2 grass\n",
            "tensor([-0.8135,  0.9404, -0.2405, -0.1350,  0.0557,  0.3363,  0.0802, -0.1015,\n",
            "        -0.5478, -0.3537,  0.0734,  0.2587,  0.1987, -0.1433,  0.2507,  0.4281,\n",
            "         0.1950,  0.5346,  0.7424,  0.0578, -0.3178,  0.9436,  0.8145, -0.0824,\n",
            "         0.6166,  0.7284, -0.3262, -1.3641,  0.1232,  0.5373, -0.5123,  0.0246,\n",
            "         1.0822, -0.2296,  0.6039,  0.5541, -0.9610,  0.4803,  0.0022,  0.5591,\n",
            "        -0.1637, -0.8468,  0.0741, -0.6216,  0.0260, -0.5162, -0.0525, -0.1418,\n",
            "        -0.0161, -0.4972, -0.5534, -0.4037,  0.5096,  1.0276, -0.0840, -1.1179,\n",
            "         0.3226,  0.4928,  0.9488,  0.2040,  0.5388,  0.8397, -0.0689,  0.3136,\n",
            "         1.0450, -0.2267, -0.0896, -0.6427,  0.6443, -1.1001, -0.0096,  0.2668,\n",
            "        -0.3230, -0.6065,  0.0479, -0.1664,  0.8571,  0.2335,  0.2539,  1.2546,\n",
            "         0.5472, -0.1980, -0.7186,  0.2076, -0.2587, -0.3650,  0.0834,  0.6932,\n",
            "         0.1574,  1.0931,  0.0913, -1.3773, -0.2717,  0.7071,  0.1872, -0.3307,\n",
            "        -0.2836,  0.1030,  1.2228,  0.8374])\n",
            "Token: 3 is\n",
            "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
            "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
            "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
            "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
            "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
            "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
            "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
            "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
            "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
            "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
            "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
            "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
            "        -0.0442, -1.2969,  0.7622,  0.4635])\n",
            "Token: 4 green\n",
            "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01, -9.9652e-01,  7.3782e-01,\n",
            "        -6.5911e-04,  2.8010e-01,  1.7287e-02, -3.6063e-01,  3.6955e-02,\n",
            "        -4.0395e-01,  2.4092e-02,  2.8958e-01,  4.0497e-01,  6.9992e-01,\n",
            "         2.5269e-01,  8.0350e-01,  4.9370e-02,  1.5562e-01, -6.3286e-03,\n",
            "        -2.9414e-01,  1.4728e-01,  1.8977e-01, -5.1791e-01,  3.6986e-01,\n",
            "         7.4582e-01,  8.2689e-02, -7.2601e-01, -4.0939e-01, -9.7822e-02,\n",
            "        -1.4096e-01,  7.1121e-01,  6.1933e-01, -2.5014e-01,  4.2250e-01,\n",
            "         4.8458e-01, -5.1915e-01,  7.7125e-01,  3.6685e-01,  4.9652e-01,\n",
            "        -4.1298e-02, -1.4683e+00,  2.0038e-01,  1.8591e-01,  4.9860e-02,\n",
            "        -1.7523e-01, -3.5528e-01,  9.4153e-01, -1.1898e-01, -5.1903e-01,\n",
            "        -1.1887e-02, -3.9186e-01, -1.7479e-01,  9.3451e-01, -5.8931e-01,\n",
            "        -2.7701e+00,  3.4522e-01,  8.6533e-01,  1.0808e+00, -1.0291e-01,\n",
            "        -9.1220e-02,  5.5092e-01, -3.9473e-01,  5.3676e-01,  1.0383e+00,\n",
            "        -4.0658e-01,  2.4590e-01, -2.6797e-01, -2.6036e-01, -1.4151e-01,\n",
            "        -1.2022e-01,  1.6234e-01, -7.4320e-01, -6.4728e-01,  4.7133e-02,\n",
            "         5.1642e-01,  1.9898e-01,  2.3919e-01,  1.2550e-01,  2.2471e-01,\n",
            "         8.2613e-01,  7.8328e-02, -5.7020e-01,  2.3934e-02, -1.5410e-01,\n",
            "        -2.5739e-01,  4.1262e-01, -4.6967e-01,  8.7914e-01,  7.2629e-01,\n",
            "         5.3862e-02, -1.1575e+00, -4.7835e-01,  2.0139e-01, -1.0051e+00,\n",
            "         1.1515e-01, -9.6609e-01,  1.2960e-01,  1.8388e-01, -3.0383e-02])\n",
            "Token: 5 .\n",
            "tensor([-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598,\n",
            "         0.4662, -0.0192,  0.4148, -0.3435,  0.2687,  0.0446,  0.4213, -0.4103,\n",
            "         0.1546,  0.0222, -0.6465,  0.2526,  0.0431, -0.1945,  0.4652,  0.4565,\n",
            "         0.6859,  0.0913,  0.2188, -0.7035,  0.1679, -0.3508, -0.1263,  0.6638,\n",
            "        -0.2582,  0.0365, -0.1361,  0.4025,  0.1429,  0.3813, -0.1228, -0.4589,\n",
            "        -0.2528, -0.3043, -0.1121, -0.2618, -0.2248, -0.4455,  0.2991, -0.8561,\n",
            "        -0.1450, -0.4909,  0.0083, -0.1749,  0.2752,  1.4401, -0.2124, -2.8435,\n",
            "        -0.2796, -0.4572,  1.6386,  0.7881, -0.5526,  0.6500,  0.0864,  0.3901,\n",
            "         1.0632, -0.3538,  0.4833,  0.3460,  0.8417,  0.0987, -0.2421, -0.2705,\n",
            "         0.0453, -0.4015,  0.1139,  0.0062,  0.0367,  0.0185, -1.0213, -0.2081,\n",
            "         0.6407, -0.0688, -0.5864,  0.3348, -1.1432, -0.1148, -0.2509, -0.4591,\n",
            "        -0.0968, -0.1795, -0.0634, -0.6741, -0.0689,  0.5360, -0.8777,  0.3180,\n",
            "        -0.3924, -0.2339,  0.4730, -0.0288])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ypfayeEnJWAw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This prints out the tokens and their embeddings. GloVe embeddings are Pytorch vectors of dimensionality 100.\n",
        "\n",
        "You choose which pre-trained embeddings you load by passing the appropriate id string to the constructor of the WordEmbeddings class. Typically, you use the two-letter language code to init an embedding, so 'en' for English and 'de' for German and so on. By default, this will initialize FastText embeddings trained over Wikipedia. You can also always use FastText embeddings over Web crawls, by instantiating with '-crawl'. So 'de-crawl' to use embeddings trained over German web crawls.\n",
        "\n",
        "For English, we provide a few more options, so here you can choose between instantiating 'en-glove', 'en-extvec' and so on."
      ]
    },
    {
      "metadata": {
        "id": "pYE-CkzHKqje",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Supported Embeddings in Flair"
      ]
    },
    {
      "metadata": {
        "id": "relukLOMJue1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| ID | Language | Embedding | \n",
        "| ------------- | -------------  | ------------- |\n",
        "| 'en-glove' (or 'glove') | English | GloVe embeddings |\n",
        "| 'en-extvec' (or 'extvec') | English |Komnios embeddings |\n",
        "| 'en-crawl' (or 'crawl')  | English | FastText embeddings over Web crawls |\n",
        "| 'en-twitter' (or 'twitter')  | English | Twitter embeddings |\n",
        "| 'en' (or 'en-news' or 'news')  |English | FastText embeddings over news and wikipedia data |\n",
        "| 'de' | German |German FastText embeddings |\n",
        "| 'nl' | Dutch | Dutch FastText embeddings |\n",
        "| 'fr' | French | French FastText embeddings |\n",
        "| 'it' | Italian | Italian FastText embeddings |\n",
        "| 'es' | Spanish | Spanish FastText embeddings |\n",
        "| 'pt' | Portuguese | Portuguese FastText embeddings |\n",
        "| 'ro' | Romanian | Romanian FastText embeddings |\n",
        "| 'ca' | Catalan | Catalan FastText embeddings |\n",
        "| 'sv' | Swedish | Swedish FastText embeddings |\n",
        "| 'da' | Danish | Danish FastText embeddings |\n",
        "| 'no' | Norwegian | Norwegian FastText embeddings |\n",
        "| 'fi' | Finnish | Finnish FastText embeddings |\n",
        "| 'pl' | Polish | Polish FastText embeddings |\n",
        "| 'cz' | Czech | Czech FastText embeddings |\n",
        "| 'sk' | Slovak | Slovak FastText embeddings |\n",
        "| 'pl' | Slovenian | Slovenian FastText embeddings |\n",
        "| 'sr' | Serbian | Serbian FastText embeddings |\n",
        "| 'hr' | Croatian | Croatian FastText embeddings |\n",
        "| 'bg' | Bulgarian | CroatBulgarianian FastText embeddings |\n",
        "| 'ru' | Russian | Russian FastText embeddings |\n",
        "| 'ar' | Arabic | Arabic FastText embeddings |\n",
        "| 'he' | Hebrew | Hebrew FastText embeddings |\n",
        "| 'tr' | Turkish | Turkish FastText embeddings |\n",
        "| 'pa' | Persian | Persian FastText embeddings |\n",
        "| 'ja' | Japanese | Japanese FastText embeddings |\n",
        "| 'ko' | Korean | Korean FastText embeddings |\n",
        "| 'zh' | Chinese | Chinese FastText embeddings |\n",
        "| 'hi' | Hindi | Hindi FastText embeddings |\n",
        "| 'id' | Indonesian | Indonesian FastText embeddings |\n",
        "| 'eu' | Basque | Basque FastText embeddings |\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ew2GOQ6NMnSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#So, if you want to load German FastText embeddings, instantiate as follows:\n",
        "\n",
        "german_embedding = WordEmbeddings('de')\n",
        "\n",
        "#Alternatively, if you want to load German FastText embeddings trained over crawls, instantiate as follows:\n",
        "\n",
        "german_embedding = WordEmbeddings('de-crawl')\n",
        "\n",
        "#We generally recommend the FastText embeddings, or GloVe if you want a smaller model.\n",
        "\n",
        "#If you want to use any other embeddings (not listed in the list above), you can load those by calling\n",
        "#custom_embedding = WordEmbeddings('path/to/your/custom/embeddings.gensim')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vT-Pe5joLXmw"
      },
      "cell_type": "markdown",
      "source": [
        "If you want to load custom embeddings you need to make sure, that the custom embeddings are correctly formatted to\n",
        "[gensim](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "\n",
        "You can, for example, convert [FastText embeddings](https://fasttext.cc/docs/en/crawl-vectors.html) to gensim using the\n",
        "following code snippet:\n"
      ]
    },
    {
      "metadata": {
        "id": "ZvDwNY3RMW4N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "#word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/path/to/fasttext/embeddings.txt', binary=False)\n",
        "#word_vectors.save('/path/to/converted')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jgm-XWpMLfU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "german_embedding = WordEmbeddings('de')\n",
        "german_embedding = WordEmbeddings('de-crawl')\n",
        "#custom_embedding = WordEmbeddings('path/to/your/custom/embeddings.gensim')\n",
        "\n",
        "import gensim\n",
        "\n",
        "#word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/path/to/fasttext/embeddings.txt', binary=False)\n",
        "#word_vectors.save('/path/to/converted')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2OEW0lIL1CZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Character Embeddings\n",
        "\n",
        "Some embeddings - such as character-features - are not pre-trained but rather trained on the downstream task. Normally\n",
        "this requires you to implement a [hierarchical embedding architecture](http://neuroner.com/NeuroNERengine_with_caption_no_figure.png). \n",
        "\n",
        "With Flair, you don't need to worry about such things. Just choose the appropriate\n",
        "embedding class and character features will then automatically train during downstream task training. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rOEyQEl3L-Hi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''from flair.embeddings import CharacterEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embedding = CharacterEmbeddings()\n",
        "\n",
        "# create a sentence\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# embed words in sentence\n",
        "embedding.embed(sentence) \n",
        "'''\n",
        "\n",
        "#CUDA errors to be addressed from version 0.4.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0I28Oks7MXCD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stacked Embeddings\n",
        "\n",
        "Stacked embeddings are one of the most important concepts of this library. You can use them to combine different\n",
        "embeddings together, for instance if you want to use both traditional embeddings together with contextual sting\n",
        "embeddings (see next chapter).\n",
        "Stacked embeddings allow you to mix and match. We find that a combination of embeddings often gives best results. \n",
        "\n",
        "All you need to do is use the `StackedEmbeddings` class and instantiate it by passing a list of embeddings that you wish \n",
        "to combine. For instance, lets combine classic GloVe embeddings with character embeddings. This is effectively the architecture proposed in (Lample et al., 2016).\n",
        "\n",
        "First, instantiate the two embeddings you wish to combine: \n"
      ]
    },
    {
      "metadata": {
        "id": "uUGKQgciMYM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings, CharacterEmbeddings\n",
        "\n",
        "# init standard GloVe embedding\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "\n",
        "# init standard character embeddings\n",
        "character_embeddings = CharacterEmbeddings()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ORkURAWM7A6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now instantiate the StackedEmbeddings class and pass it a list containing these two embeddings.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8hMn9Mf8M1zy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import StackedEmbeddings\n",
        "\n",
        "# now create the StackedEmbedding object that combines all embeddings\n",
        "stacked_embeddings = StackedEmbeddings(\n",
        "    embeddings=[glove_embedding, character_embeddings])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iPt1xxiCNJeF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's it! Now just use this embedding like all the other embeddings, i.e. call the `embed()` method over your sentences.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_heA_Ug5M_-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
        "stacked_embeddings.embed(sentence)\n",
        "\n",
        "# now check out the embedded tokens.\n",
        "for token in sentence:\n",
        "    print(token)\n",
        "    print(token.embedding)\n",
        "#RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 'index' .  This error will be removed from Flair==0.4.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NuOA52QONVeK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Words are now embedded using a concatenation of two different embeddings. This means that the resulting embedding\n",
        "vector is still a single Pytorch vector. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dS4vDq8rKWGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Next \n",
        "\n",
        "You can now either look into [BERT, ELMo, and Flair embeddings](/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md),\n",
        "or go directly to the tutorial about [loading your corpus](/resources/docs/TUTORIAL_6_CORPUS.md), which is a\n",
        "pre-requirement for [training your own models](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md).\n",
        "\n",
        "Drag and Drop\n",
        "The image will be downloaded by Fatkun"
      ]
    },
    {
      "metadata": {
        "id": "dEWvkQosNZCe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**In next column we will check about using the Bert, Elmo and Flair Embeddings**"
      ]
    },
    {
      "metadata": {
        "id": "0aikssTCyc2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4. Using Bert, Elmo and Flair Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "3yj878ForV8g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next to standard WordEmbeddings and CharacterEmbeddings, we also provide classes for BERT, ELMo and Flair embeddings. These embeddings enable you to train truly state-of-the-art NLP models.\n",
        "\n",
        "This tutorial explains how to use these embeddings. We assume that you're familiar with the [base types](/resources/docs/TUTORIAL_1_BASICS.md) of this library as well as [standard word embeddings](/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md), in particular the `StackedEmbeddings` class.\n"
      ]
    },
    {
      "metadata": {
        "id": "vGq7INmWxZpP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "All word embedding classes inherit from the `TokenEmbeddings` class and implement the `embed()` method which you need to\n",
        "call to embed your text. This means that for most users of Flair, the complexity of different embeddings remains\n",
        "hidden behind this interface. Simply instantiate the embedding class you require and call `embed()` to embed your text.\n",
        "\n",
        "All embeddings produced with our methods are Pytorch vectors, so they can be immediately used for training and\n",
        "fine-tuning."
      ]
    },
    {
      "metadata": {
        "id": "_B7IDN0Exiew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Flair Embeddings\n",
        "\n",
        "Contextual string embeddings are [powerful embeddings](https://drive.google.com/file/d/17yVpFA7MmXaQFTe-HDpZuqw9fJlmzg56/view?usp=sharing)\n",
        " that capture latent syntactic-semantic information that goes beyond\n",
        "standard word embeddings. Key differences are: (1) they are trained without any explicit notion of words and\n",
        "thus fundamentally model words as sequences of characters. And (2) they are **contextualized** by their\n",
        "surrounding text, meaning that the *same word will have different embeddings depending on its\n",
        "contextual use*.\n",
        "\n",
        "With Flair, you can use these embeddings simply by instantiating the appropriate embedding class, same as standard word embeddings:\n"
      ]
    },
    {
      "metadata": {
        "id": "gsedyJ27tsVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "203d8c04-2de0-4d20-ed6a-5d835248e642"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# init embedding\n",
        "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
        "\n",
        "# create a sentence\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# embed words in sentence\n",
        "flair_embedding_forward.embed(sentence)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"The grass is green .\" - 5 Tokens]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "MOGcSX9Rxt5e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You choose which embeddings you load by passing the appropriate string to the constructor of the `FlairEmbeddings` class. \n",
        "Currently, the following contextual string embeddings are provided (more coming):\n",
        " \n",
        "| ID | Language | Embedding | \n",
        "| -------------     | ------------- | ------------- |\n",
        "| 'multi-forward'    | English, German, French, Italian, Dutch, Polish | Mix of corpora (Web, Wikipedia, Subtitles, News) |\n",
        "| 'multi-backward'    | English, German, French, Italian, Dutch, Polish | Mix of corpora (Web, Wikipedia, Subtitles, News) |\n",
        "| 'multi-forward-fast'    | English, German, French, Italian, Dutch, Polish | Mix of corpora (Web, Wikipedia, Subtitles, News) |\n",
        "| 'multi-backward-fast'    | English, German, French, Italian, Dutch, Polish | Mix of corpora (Web, Wikipedia, Subtitles, News) |\n",
        "| 'news-forward'    | English | Forward LM embeddings over 1 billion word corpus |\n",
        "| 'news-backward'   | English | Backward LM embeddings over 1 billion word corpus |\n",
        "| 'news-forward-fast'    | English | Smaller, CPU-friendly forward LM embeddings over 1 billion word corpus |\n",
        "| 'news-backward-fast'   | English | Smaller, CPU-friendly backward LM embeddings over 1 billion word corpus |\n",
        "| 'mix-forward'     | English | Forward LM embeddings over mixed corpus (Web, Wikipedia, Subtitles) |\n",
        "| 'mix-backward'    | English | Backward LM embeddings over mixed corpus (Web, Wikipedia, Subtitles) |\n",
        "| 'german-forward'  | German  | Forward LM embeddings over mixed corpus (Web, Wikipedia, Subtitles) |\n",
        "| 'german-backward' | German  | Backward LM embeddings over mixed corpus (Web, Wikipedia, Subtitles) |\n",
        "| 'polish-forward'  | Polish  | Added by [@borchmann](https://github.com/applicaai/poleval-2018): Forward LM embeddings over web crawls (Polish part of CommonCrawl) |\n",
        "| 'polish-backward' | Polish  | Added by [@borchmann](https://github.com/applicaai/poleval-2018): Backward LM embeddings over web crawls (Polish part of CommonCrawl) |\n",
        "| 'slovenian-forward'  | Slovenian  | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Forward LM embeddings over various sources (Europarl, Wikipedia and OpenSubtitles2018) |\n",
        "| 'slovenian-backward' | Slovenian  | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Backward LM embeddings over various sources (Europarl, Wikipedia and OpenSubtitles2018) |\n",
        "| 'bulgarian-forward'  | Bulgarian  | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Forward LM embeddings over various sources (Europarl, Wikipedia or SETimes) |\n",
        "| 'bulgarian-backward' | Bulgarian  | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Backward LM embeddings over various sources (Europarl, Wikipedia or SETimes) |\n",
        "| 'dutch-forward'    | Dutch | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Forward LM embeddings over various sources (Europarl, Wikipedia or OpenSubtitles2018) |\n",
        "| 'dutch-backward'    | Dutch | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Backward LM embeddings over various sources (Europarl, Wikipedia or OpenSubtitles2018) |\n",
        "| 'swedish-forward'    | Swedish | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Forward LM embeddings over various sources (Europarl, Wikipedia or OpenSubtitles2018) |\n",
        "| 'swedish-backward'    | Swedish | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Backward LM embeddings over various sources (Europarl, Wikipedia or OpenSubtitles2018) |\n",
        "| 'french-forward'    | French | Added by [@mhham](https://github.com/mhham): Forward LM embeddings over French Wikipedia |\n",
        "| 'french-backward'    | French | Added by [@mhham](https://github.com/mhham): Backward LM embeddings over French Wikipedia |\n",
        "| 'czech-forward'    | Czech | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Forward LM embeddings over various sources (Europarl, Wikipedia or OpenSubtitles2018) |\n",
        "| 'czech-backward'    | Czech | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Backward LM embeddings over various sources (Europarl, Wikipedia or OpenSubtitles2018) |\n",
        "| 'portuguese-forward'    | Portuguese | Added by [@ericlief](https://github.com/ericlief/language_models): Forward LM embeddings |\n",
        "| 'portuguese-backward'    | Portuguese | Added by [@ericlief](https://github.com/ericlief/language_models): Backward LM embeddings |\n",
        "| 'basque-forward'    | Basque | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Forward LM embeddings |\n",
        "| 'basque-backward'    | Basque | Added by [@stefan-it](https://github.com/stefan-it/flair-lms): Backward LM embeddings |\n",
        "| 'spanish-forward'    | Spanish | Added by [@iamyihwa](https://github.com/zalandoresearch/flair/issues/80): Forward LM embeddings over Wikipedia |\n",
        "| 'spanish-backward'    | Spanish | Added by [@iamyihwa](https://github.com/zalandoresearch/flair/issues/80): Backward LM embeddings over Wikipedia |\n",
        "| 'spanish-forward-fast'    | Spanish | Added by [@iamyihwa](https://github.com/zalandoresearch/flair/issues/80): CPU-friendly forward LM embeddings over Wikipedia |\n",
        "| 'spanish-backward-fast'    | Spanish | Added by [@iamyihwa](https://github.com/zalandoresearch/flair/issues/80): CPU-friendly backward LM embeddings over Wikipedia |\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XDcsi9gGyH-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#So, if you want to load embeddings from the English news backward LM model, instantiate the method as follows:\n",
        "\n",
        "flair_backward = FlairEmbeddings('news-backward')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCsw2Hy2yM8g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recommended Flair Usage\n",
        "\n",
        "We recommend combining both forward and backward Flair embeddings. Depending on the task, we also recommend adding standard word embeddings into the mix. So, our recommended `StackedEmbedding` for most English tasks is: \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KD_jpRjEySdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
        "\n",
        "# create a StackedEmbedding object that combines glove and forward/backward flair embeddings\n",
        "stacked_embeddings = StackedEmbeddings([\n",
        "                                        WordEmbeddings('glove'), \n",
        "                                        FlairEmbeddings('news-forward'), \n",
        "                                        FlairEmbeddings('news-backward'),\n",
        "                                       ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "181tmcn9yxm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c09b0e53-8edb-470f-ac02-12bd4c8ce5fa"
      },
      "cell_type": "code",
      "source": [
        "#That's it! Now just use this embedding like all the other embeddings, i.e. call the `embed()` method over your sentences.\n",
        "\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
        "stacked_embeddings.embed(sentence)\n",
        "\n",
        "# now check out the embedded tokens.\n",
        "for token in sentence:\n",
        "    print(token)\n",
        "    print(token.embedding)\n",
        "\n",
        "#Words are now embedded using a concatenation of three different embeddings. This combination often gives state-of-the-art accuracy.\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: 1 The\n",
            "tensor([-3.8194e-02, -2.4487e-01,  7.2812e-01,  ..., -2.5692e-05,\n",
            "        -5.9604e-03, -2.5547e-03])\n",
            "Token: 2 grass\n",
            "tensor([-8.1353e-01,  9.4042e-01, -2.4048e-01,  ..., -6.7730e-05,\n",
            "        -3.0360e-03, -1.3282e-02])\n",
            "Token: 3 is\n",
            "tensor([-0.5426,  0.4148,  1.0322,  ..., -0.0066, -0.0036, -0.0014])\n",
            "Token: 4 green\n",
            "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01,  ..., -2.2563e-05,\n",
            "        -1.0894e-04, -4.3916e-03])\n",
            "Token: 5 .\n",
            "tensor([-3.3979e-01,  2.0941e-01,  4.6348e-01,  ...,  4.1382e-05,\n",
            "        -4.4364e-04, -2.5425e-02])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZA_KoakuzFE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## BERT Embeddings\n",
        "\n",
        "[BERT embeddings](https://arxiv.org/pdf/1810.04805.pdf) were developed by Devlin et al. (2018) and are a different kind\n",
        "of powerful word embedding based on a bidirectional transformer architecture.\n",
        "We are using the implementation of [huggingface](https://github.com/huggingface/pytorch-pretrained-BERT) in Flair.\n",
        "The embeddings itself are wrapped into our simple embedding interface, so that they can be used like any other\n",
        "embedding.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tDWdjeGCzHni",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "from flair.embeddings import BertEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embedding = BertEmbeddings()\n",
        "\n",
        "# create a sentence\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# embed words in sentence\n",
        "embedding.embed(sentence)\n",
        "\n",
        "#RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 'index' wait for flair==0.4.1'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pCm3aPyIzOlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can load any of the pre-trained BERT models by providing the model string during initialization:\n",
        "\n",
        "| ID | Language | Embedding |\n",
        "| -------------     | ------------- | ------------- |\n",
        "| 'bert-base-uncased' | English | 12-layer, 768-hidden, 12-heads, 110M parameters |\n",
        "| 'bert-large-uncased'   | English | 24-layer, 1024-hidden, 16-heads, 340M parameters |\n",
        "| 'bert-base-cased'    | English | 12-layer, 768-hidden, 12-heads , 110M parameters |\n",
        "| 'bert-large-cased'   | English | 24-layer, 1024-hidden, 16-heads, 340M parameters |\n",
        "| 'bert-base-multilingual-cased'     | 104 languages | 12-layer, 768-hidden, 12-heads, 110M parameters |\n",
        "| 'bert-base-chinese'    | Chinese Simplified and Traditional | 12-layer, 768-hidden, 12-heads, 110M parameters |\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EATVXVOMzTmH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##ELMo Embeddings\n",
        "ELMo embeddings were presented by Peters et al. in 2018. They are using a bidirectional recurrent neural network to predict the next word in a text. We are using the implementation of AllenNLP. As this implementation comes with a lot of sub-dependencies, which we don't want to include in Flair, you need to first install the library via pip install allennlp before you can use it in Flair. Using the embeddings is as simple as using any other embedding type:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dS40oAUZFd3o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZa0qSZvzah3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d70c28a0-dfce-4e2a-cdda-05da96fb3f9d"
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import ELMoEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embedding = ELMoEmbeddings()\n",
        "\n",
        "# create a sentence\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# embed words in sentence\n",
        "embedding.embed(sentence)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"The grass is green .\" - 5 Tokens]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "hvXYcUyJzfyT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "AllenNLP provides the following pre-trained models. To use any of the following models inside Flair\n",
        "simple specify the embedding id when initializing the `ELMoEmbeddings`.\n",
        "\n",
        "| ID | Language | Embedding |\n",
        "| ------------- | ------------- | ------------- |\n",
        "| 'small' | English | 1024-hidden, 1 layer, 14.6M parameters |\n",
        "| 'medium'   | English | 2048-hidden, 1 layer, 28.0M parameters |\n",
        "| 'original'    | English | 4096-hidden, 2 layers, 93.6M parameters |\n",
        "| 'pt'   | Portuguese | |\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kTZJGMggzsA-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Combining BERT and Flair\n",
        "\n",
        "You can very easily mix and match Flair, ELMo, BERT and classic word embeddings. All you need to do is instantiate each embedding you wish to combine and use them in a StackedEmbedding. \n",
        "\n",
        "For instance, let's say we want to combine the multilingual Flair and BERT embeddings to train a hyper-powerful multilingual downstream task model. \n",
        "\n",
        "First, instantiate the embeddings you wish to combine: \n"
      ]
    },
    {
      "metadata": {
        "id": "rBCyZl-jzl_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import FlairEmbeddings, BertEmbeddings\n",
        "\n",
        "# init Flair embeddings\n",
        "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
        "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
        "\n",
        "# init multilingual BERT\n",
        "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVOYU4sCzxFi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Now instantiate the StackedEmbeddings class and pass it a list containing these three embeddings.\n",
        "\n",
        "from flair.embeddings import StackedEmbeddings\n",
        "\n",
        "# now create the StackedEmbedding object that combines all embeddings\n",
        "stacked_embeddings = StackedEmbeddings(\n",
        "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYAHwf3Yz5al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#That's it! Now just use this embedding like all the other embeddings, i.e. call the `embed()` method over your sentences.\n",
        "\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
        "stacked_embeddings.embed(sentence)\n",
        "\n",
        "# now check out the embedded tokens.\n",
        "for token in sentence:\n",
        "    print(token)\n",
        "    print(token.embedding)\n",
        "#RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 'index'\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VOQ_LaDm0BL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Words are now embedded using a concatenation of three different embeddings. This means that the resulting embedding vector is still a single Pytorch vector. \n"
      ]
    },
    {
      "metadata": {
        "id": "aukozSwMtGtC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Next \n",
        "\n",
        "You can now either look into [document embeddings](/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md) to embed entire text \n",
        "passages with one vector for tasks such as text classification, or go directly to the tutorial about \n",
        "[loading your corpus](/resources/docs/TUTORIAL_6_CORPUS.md), which is a pre-requirement for\n",
        "[training your own models](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md).\n",
        "\n",
        "Drag and Drop\n",
        "The image will be downloaded by Fatkun"
      ]
    },
    {
      "metadata": {
        "id": "zoxEHlxRyd3W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#5. Using Document Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "GlOIVYL691A6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Document embeddings are different from [word embeddings](/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md) in that they \n",
        "give you one embedding for an entire text, whereas word embeddings give you embeddings for individual words. \n",
        "\n",
        "For this tutorial, we assume that you're familiar with the [base types](/resources/docs/TUTORIAL_1_BASICS.md) of this\n",
        "library and how [word embeddings](/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md) work.\n"
      ]
    },
    {
      "metadata": {
        "id": "1dir-Juj96o-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Embeddings\n",
        "\n",
        "All document embedding classes inherit from the `DocumentEmbeddings` class and implement the `embed()` method which you\n",
        "need to call to embed your text. This means that for most users of Flair, the complexity of different embeddings remains\n",
        "hidden behind this interface. Simply instantiate the embedding class you require and call `embed()` to embed your text.\n",
        "\n",
        "All embeddings produced with our methods are Pytorch vectors, so they can be immediately used for training and\n",
        "fine-tuning.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F2ii1D6s-AZ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Document Embeddings\n",
        "\n",
        "Our document embeddings are created from the embeddings of all words in the document.\n",
        "Currently, we have two different methods to obtain a document embedding from a list of word embeddings.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_MbT9mD1-BLv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pooling\n",
        "\n",
        "The first method calculates a pooling operation over all word embeddings in a document.\n",
        "The default operation is `mean` which gives us the mean of all words in the sentence.\n",
        "The resulting embedding is taken as document embedding.\n",
        "\n",
        "To create a mean document embedding simply create any number of `TokenEmbeddings` first and put them in a list.\n",
        "Afterwards, initiate the `DocumentPoolEmbeddings` with this list of `TokenEmbeddings`.\n",
        "So, if you want to create a document embedding using GloVe embeddings together with CharLMEmbeddings,\n",
        "use the following code:\n"
      ]
    },
    {
      "metadata": {
        "id": "RZIrKk3O-GCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence\n",
        "\n",
        "# initialize the word embeddings\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
        "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
        "\n",
        "# initialize the document embeddings, mode = mean\n",
        "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
        "                                              flair_embedding_backward,\n",
        "                                              flair_embedding_forward])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3aQjOYk-LvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2562a36b-2110-49b7-b87b-24eb14b3e85d"
      },
      "cell_type": "code",
      "source": [
        "#Now, create an example sentence and call the embedding's `embed()` method.\n",
        "\n",
        "# create an example sentence\n",
        "sentence = Sentence('The grass is green . And the sky is blue .')\n",
        "\n",
        "# embed the sentence with our document embedding\n",
        "document_embeddings.embed(sentence)\n",
        "\n",
        "# now check out the embedded sentence.\n",
        "print(sentence.get_embedding())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.3197,  0.2621,  0.4037,  ..., -0.0008, -0.0051, -0.0109]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dWkT1lTK-R4l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This prints out the embedding of the document.\n",
        "Since the document embedding is derived from word embeddings, its dimensionality depends on the dimensionality of word\n",
        "embeddings you are using.\n",
        "\n",
        "Next to the `mean` pooling operation you can also use `min` or `max` pooling. Simply pass the pooling operation you want\n",
        "to use to the initialization of the `DocumentPoolEmbeddings`:\n"
      ]
    },
    {
      "metadata": {
        "id": "WxHfOyV5-T3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
        "                                             flair_embedding_backward,\n",
        "                                             flair_embedding_backward],\n",
        "                                             mode='min')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2l33Yt1M-W7e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### LSTM\n",
        "\n",
        "Besides the pooling we also support a method based on an LSTM to obtain a `DocumentEmbeddings`.\n",
        "The LSTM takes the word embeddings of every token in the document as input and provides its last output state as document\n",
        "embedding.\n",
        "\n",
        "In order to use the `DocumentLSTMEmbeddings` you need to initialize them by passing a list of token embeddings to it:\n"
      ]
    },
    {
      "metadata": {
        "id": "LRsUHrBn-bAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings, DocumentLSTMEmbeddings\n",
        "\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "\n",
        "document_embeddings = DocumentLSTMEmbeddings([glove_embedding])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "erjt9T1q-foR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e260e72e-5e3e-4b95-e1b4-b68c82e116bd"
      },
      "cell_type": "code",
      "source": [
        "#Now, create an example sentence and call the embedding's `embed()` method.\n",
        "\n",
        "# create an example sentence\n",
        "sentence = Sentence('The grass is green . And the sky is blue .')\n",
        "\n",
        "# embed the sentence with our document embedding\n",
        "document_embeddings.embed(sentence)\n",
        "\n",
        "# now check out the embedded sentence.\n",
        "print(sentence.get_embedding())\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2423,  0.0000,  0.0346,  0.0000,  0.0000, -0.2645,  0.0000,  0.7164,\n",
            "         -0.4283,  0.0000,  0.0000,  0.1647,  0.3708,  0.0000,  0.0000, -0.5073,\n",
            "          0.3975,  0.0000,  0.0644,  0.0000, -0.7402, -0.2886, -0.8609,  0.0000,\n",
            "          0.0000,  0.2623,  0.1021,  0.0000,  0.0000,  0.5669,  0.0000,  0.4927,\n",
            "         -0.1687, -0.3182,  0.4296,  0.0000,  0.0000,  0.5056,  0.2128,  0.0000,\n",
            "          0.0000,  0.2560,  0.0000,  0.0000,  0.0000, -0.8810, -0.4320,  0.0000,\n",
            "          0.5811,  0.0000, -0.2491, -0.3894, -0.5912,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.1569,  0.0000,  0.1381, -0.1146,  0.0000,  0.0000,\n",
            "         -0.0342, -0.0795,  0.0000,  0.0000,  0.4960, -0.7861,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.3867,  0.0000, -0.7379, -0.5240,  1.0969,\n",
            "          0.0000,  0.5494, -0.2198,  0.0000,  0.0000,  0.0000,  0.5212,  0.4519,\n",
            "          0.0000,  0.0000,  0.7530,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.1180,  0.3933,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.4400,  0.0000,  0.1744,  0.0000,  0.0000, -0.0029,  0.2286,  0.0000,\n",
            "          0.3957, -0.3927, -0.0543,  0.0000,  0.7131, -0.7090,  0.2315,  0.4632,\n",
            "          0.0000,  0.2288,  1.2253, -0.0016,  0.0000,  0.7459, -0.8121,  0.0000]],\n",
            "       grad_fn=<CatBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5SYDj3kE0QRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will output a single embedding for the complete sentence. The embedding dimensionality depends on the number of\n",
        "hidden states you are using and whether the LSTM is bidirectional or not.\n",
        "\n",
        "Note that while `DocumentPoolEmbeddings` are immediately meaningful, `DocumentLSTMEmbeddings` need to be tuned on the\n",
        "downstream task. This happens automatically in Flair if you train a new model with these embeddings.\n",
        "`DocumentLSTMEmbeddings` have a number of hyper-parameters that can be tuned to improve learning:\n",
        "\n",
        "```text\n",
        ":param hidden_size: the number of hidden states in the lstm.\n",
        ":param rnn_layers: the number of layers for the lstm.\n",
        ":param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n",
        "layer before putting them into the lstm or not.\n",
        ":param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n",
        "dimension as before will be taken.\n",
        ":param bidirectional: boolean value, indicating whether to use a bidirectional lstm or not.\n",
        ":param dropout: the dropout value to be used.\n",
        ":param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used.\n",
        ":param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OgqKd1Rl-ojW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Next \n",
        "\n",
        "You can now either look into the tutorial about [loading your corpus](/resources/docs/TUTORIAL_6_CORPUS.md), which\n",
        "is a pre-requirement for [training your own models](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md)\n",
        "or into [training your own embeddings](/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md).\n",
        "Drag and Drop\n",
        "The image will be downloaded by Fatkun"
      ]
    },
    {
      "metadata": {
        "id": "7f9A2uygyd_g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#6. Loading your own Corpus"
      ]
    },
    {
      "metadata": {
        "id": "nkdVSupL-74v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part of the tutorial shows how you can load your own corpus for training your own model later on.\n",
        "\n",
        "For this tutorial, we assume that you're familiar with the [base types](/resources/docs/TUTORIAL_1_BASICS.md) of this\n",
        "library."
      ]
    },
    {
      "metadata": {
        "id": "iothwiKW_CSa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reading A Sequence Labeling Dataset\n",
        "\n",
        "Most sequence labeling datasets in NLP use some sort of column format in which each line is a word and each column is\n",
        "one level of linguistic annotation. See for instance this sentence:\n",
        "\n",
        "```console\n",
        "George N B-PER\n",
        "Washington N I-PER\n",
        "went V O\n",
        "to P O\n",
        "Washington N B-LOC\n",
        "```\n",
        "\n",
        "The first column is the word itself, the second coarse PoS tags, and the third BIO-annotated NER tags. To read such a \n",
        "dataset, define the column structure as a dictionary and use a helper method.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "o9OTrQwB_I7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import TaggedCorpus\n",
        "'''\n",
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "\n",
        "# define columns\n",
        "columns = {0: 'text', 1: 'pos', 2: 'ner'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder = '/path/to/data/folder'\n",
        "\n",
        "# retrieve corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
        "                                                              train_file='train.txt',\n",
        "                                                              test_file='test.txt',\n",
        "                                                              dev_file='dev.txt')\n",
        "                                                              \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOLFx0yJ_NKh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "This gives you a `TaggedCorpus` object that contains the train, dev and test splits, each has a list of `Sentence`.\n",
        "So, to check how many sentences there are in the training split, do"
      ]
    },
    {
      "metadata": {
        "id": "IYMPtOQU_W6A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#len(corpus.train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "neQNcTTR_Z3U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also access a sentence and check out annotations. Lets assume that the first sentence in the training split is\n",
        "the example sentence from above, then executing these commands"
      ]
    },
    {
      "metadata": {
        "id": "S9MHsicM_csC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(corpus.train[0].to_tagged_string('pos'))\n",
        "#print(corpus.train[0].to_tagged_string('ner'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4i8QyC2Q_lZx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "will print the sentence with different layers of annotation:\n",
        "\n",
        "```console\n",
        "George <N> Washington <N> went <V> to <P> Washington <N>\n",
        "\n",
        "George <B-PER> Washington <I-PER> went to Washington <B-LOC> .\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "Ywze-7E4_qsa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reading a Text Classification Dataset\n",
        "\n",
        "Our text classification data format is based on the \n",
        "[FastText format](https://fasttext.cc/docs/en/supervised-tutorial.html), in which each line in the file represents a \n",
        "text document. A document can have one or multiple labels that are defined at the beginning of the line starting with \n",
        "the prefix `__label__`. This looks like this:\n",
        "\n",
        "```bash\n",
        "__label__<label_1> <text>\n",
        "__label__<label_1> __label__<label_2> <text>\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "ADw5aOmV_zqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "To create a `TaggedCorpus` for a text classification task, you need to have three files (train, dev, and test) in the \n",
        "above format located in one folder. This data folder structure could, for example, look like this for the IMDB task:\n",
        "```text\n",
        "/resources/tasks/imdb/train.txt\n",
        "/resources/tasks/imdb/dev.txt\n",
        "/resources/tasks/imdb/test.txt\n",
        "```\n",
        "If you now point the `NLPTaskDataFetcher` to this folder (`/resources/tasks/imdb`), it will create a `TaggedCorpus` out of \n",
        "the three different files. Thereby, each line in a file is converted to a `Sentence` object annotated with the labels.\n",
        "\n",
        "Attention: A text in a line can have multiple sentences. Thus, a `Sentence` object can actually consist of multiple\n",
        "sentences.\n"
      ]
    },
    {
      "metadata": {
        "id": "mhwPQUso_3S_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from pathlib import Path\n",
        "\n",
        "# use your own data path\n",
        "data_folder = Path('/resources/tasks/imdb')\n",
        "\n",
        "# load corpus containing training, test and dev data\n",
        "corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder,\n",
        "                                                                     test_file='test.txt',\n",
        "                                                                     dev_file='dev.txt',\n",
        "                                                                     train_file='train.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVQkYqEY_8TR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you just want to read a single file, you can use \n",
        "`NLPTaskDataFetcher.read_text_classification_file('path/to/file.txt)`, which returns a list of `Sentence` objects."
      ]
    },
    {
      "metadata": {
        "id": "9LG5zZ2yAAJC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading A Dataset\n",
        "\n",
        "Flair also supports a couple of datasets out of the box.\n",
        "You can simple load your preferred dataset by calling, for example\n",
        "```python\n",
        "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\n",
        "```\n",
        "This line of code will download the UD_ENGLISH dataset and puts it into `~/.flair/datasets/ud_english`.\n",
        "The method returns a `TaggedCorpus` which can be directly used to train your model."
      ]
    },
    {
      "metadata": {
        "id": "TqNmgznAAKxu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following datasets are supported:\n",
        "\n",
        "| `NLPTask` | `NLPTask` | `NLPTask` |\n",
        "|---|---|---|\n",
        "| [CONLL_2000](https://www.clips.uantwerpen.be/conll2000/chunking/) | [UD_DUTCH](https://github.com/UniversalDependencies/UD_Dutch-Alpino) | [UD_CROATIAN](https://github.com/UniversalDependencies/UD_Croatian-SET) |\n",
        "| [CONLL_03_DUTCH](https://www.clips.uantwerpen.be/conll2002/ner/) | [UD_FRENCH](https://github.com/UniversalDependencies/UD_French-GSD) | [UD_SERBIAN](https://github.com/UniversalDependencies/UD_Serbian-SET) |\n",
        "| [CONLL_03_SPANISH](https://www.clips.uantwerpen.be/conll2002/ner/) | [UD_ITALIAN](https://github.com/UniversalDependencies/UD_Italian-ISDT) | [UD_BULGARIAN](https://github.com/UniversalDependencies/UD_Bulgarian-BTB) |\n",
        "| [WNUT_17](https://noisy-text.github.io/2017/files/) | [UD_SPANISH](https://github.com/UniversalDependencies/UD_Spanish-GSD) | [UD_ARABIC](https://github.com/UniversalDependencies/UD_Arabic-PADT) |\n",
        "| [WIKINER_ENGLISH](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_PORTUGUESE](https://github.com/UniversalDependencies/UD_Portuguese-Bosque) | [UD_HEBREW](https://github.com/UniversalDependencies/UD_Hebrew-HTB) |\n",
        "| [WIKINER_GERMAN](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_ROMANIAN](https://github.com/UniversalDependencies/UD_Romanian-RRT) | [UD_TURKISH](https://github.com/UniversalDependencies/UD_Turkish-IMST) |\n",
        "| [WIKINER_DUTCH](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_CATALAN](https://github.com/UniversalDependencies/UD_Catalan-AnCora) | [UD_PERSIAN](https://github.com/UniversalDependencies/UD_Persian-Seraji) |\n",
        "| [WIKINER_FRENCH](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_POLISH](https://github.com/UniversalDependencies/UD_Polish-LFG) | [UD_RUSSIAN](https://github.com/UniversalDependencies/UD_Russian-SynTagRus) |\n",
        "| [WIKINER_ITALIAN](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_CZECH](https://github.com/UniversalDependencies/UD_Czech-PDT) | [UD_HINDI](https://github.com/UniversalDependencies/UD_Hindi-HDTB) |\n",
        "| [WIKINER_SPANISH](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_SLOVAK](https://github.com/UniversalDependencies/UD_Slovak-SNK) | [UD_INDONESIAN](https://github.com/UniversalDependencies/UD_Indonesian-GSD) |\n",
        "| [WIKINER_PORTUGUESE](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_SWEDISH](https://github.com/UniversalDependencies/UD_Swedish-Talbanken) | [UD_JAPANESE](https://github.com/UniversalDependencies/UD_Japanese-GSD) |\n",
        "| [WIKINER_POLISH](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_DANISH](https://github.com/UniversalDependencies/UD_Danish-DDT) | [UD_CHINESE](https://github.com/UniversalDependencies/UD_Chinese-GSD) |\n",
        "| [WIKINER_RUSSIAN](https://github.com/dice-group/FOX/tree/master/input/Wikiner) | [UD_NORWEGIAN](https://github.com/UniversalDependencies/UD_Norwegian-Bokmaal) | [UD_KOREAN](https://github.com/UniversalDependencies/UD_Korean-Kaist) |\n",
        "| [UD_ENGLISH](https://github.com/UniversalDependencies/UD_English-EWT) | [UD_FINNISH](https://github.com/UniversalDependencies/UD_Finnish-TDT) |  [UD_BASQUE](https://github.com/UniversalDependencies/UD_Basque-BDT) |\n",
        "| [UD_GERMAN](https://github.com/UniversalDependencies/UD_German-GSD) | [UD_SLOVENIAN](https://github.com/UniversalDependencies/UD_Slovenian-SSJ) |\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DME-wBsCAPka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The TaggedCorpus Object\n",
        "\n",
        "The `TaggedCorpus` represents your entire dataset. A `TaggedCorpus` consists of a list of `train` sentences,\n",
        "a list of `dev` sentences, and a list of `test` sentences.\n",
        "\n",
        "A `TaggedCorpus` contains a bunch of useful helper functions.\n",
        "For instance, you can downsample the data by calling `downsample()` and passing a ratio. So, if you normally get a \n",
        "corpus like this:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rJv3EVxOAY1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "original_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\n",
        "\n",
        "#then you can downsample the corpus, simply like this:\n",
        "\n",
        "downsampled_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH).downsample(0.1)\n",
        "\n",
        "#If you print both corpora, you see that the second one has been downsampled to 10% of the data.\n",
        "\n",
        "print(\"--- 1 Original ---\")\n",
        "print(original_corpus)\n",
        "\n",
        "print(\"--- 2 Downsampled ---\")\n",
        "print(downsampled_corpus)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Pj2puWoAqT-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should print:\n",
        "\n",
        "```console\n",
        "--- 1 Original ---\n",
        "TaggedCorpus: 12543 train + 2002 dev + 2077 test sentences\n",
        "\n",
        "--- 2 Downsampled ---\n",
        "TaggedCorpus: 1255 train + 201 dev + 208 test sentences\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oZ3K2ccjAt5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For many learning task you need to create a target dictionary. Thus, the `TaggedCorpus` enables you to create your\n",
        "#tag or label dictionary, depending on the task you want to learn. Simple execute the following code snippet to do so:\n",
        "\n",
        "# create tag dictionary for a PoS task\n",
        "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\n",
        "print(corpus.make_tag_dictionary('upos'))\n",
        "\n",
        "# create tag dictionary for an NER task\n",
        "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03_DUTCH)\n",
        "print(corpus.make_tag_dictionary('ner'))\n",
        "\n",
        "# create label dictionary for a text classification task\n",
        "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path='path/to/data/folder')\n",
        "print(corpus.make_label_dictionary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6GKghocAp8j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Another useful function is `obtain_statistics()` which returns you a python dictionary with useful statistics about your\n",
        "#dataset. Using it, for example, on the IMDB dataset like this\n",
        "\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        " \n",
        "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path='path/to/data/folder')\n",
        "stats = corpus.obtain_statistics()\n",
        "print(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9rb-r7xzBC9e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "outputs the following information\n",
        "\n",
        "```text\n",
        "{\n",
        "  'TRAIN': {\n",
        "    'dataset': 'TRAIN', \n",
        "    'total_number_of_documents': 25000, \n",
        "    'number_of_documents_per_class': {'POSITIVE': 12500, 'NEGATIVE': 12500}, \n",
        "    'number_of_tokens': {'total': 6868314, 'min': 10, 'max': 2786, 'avg': 274.73256}\n",
        "  }, \n",
        "  'TEST': {\n",
        "    'dataset': 'TEST', \n",
        "    'total_number_of_documents': 12500, \n",
        "    'number_of_documents_per_class': {'NEGATIVE': 6245, 'POSITIVE': 6255}, \n",
        "    'number_of_tokens': {'total': 3379510, 'min': 8, 'max': 2768, 'avg': 270.3608}\n",
        "  }, 'DEV': {\n",
        "    'dataset': 'DEV', \n",
        "    'total_number_of_documents': 12500, \n",
        "    'number_of_documents_per_class': {'POSITIVE': 6245, 'NEGATIVE': 6255}, \n",
        "    'number_of_tokens': {'total': 3334898, 'min': 7, 'max': 2574, 'avg': 266.79184}\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "UiF-lttKBFZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The MultiCorpus Object\n",
        "\n",
        "If you want to train multiple tasks at once, you can use the `MultiCorpus` object.\n",
        "To initiate the `MultiCorpus` you first need to create any number of `TaggedCorpus` objects. Afterwards, you can pass\n",
        "a list of `TaggedCorpus` to the `MultiCorpus` object.\n",
        "\n",
        "```text\n",
        "english_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\n",
        "german_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_GERMAN)\n",
        "dutch_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_DUTCH)\n",
        "\n",
        "multi_corpus = MultiCorpus([english_corpus, german_corpus, dutch_corpus])\n",
        "```\n",
        "\n",
        "The `MultiCorpus` object has the same interface as the `TaggedCorpus`.\n",
        "You can simple pass a `MultiCorpus` to a trainer instead of a `TaggedCorpus`, the trainer will not know the difference\n",
        "and training operates as usual.\n"
      ]
    },
    {
      "metadata": {
        "id": "BaPRZddo-z74",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Next\n",
        "\n",
        "You can now look into [training your own models](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md).\n",
        "Drag and Drop\n",
        "The image will be downloaded by Fatkun."
      ]
    },
    {
      "metadata": {
        "id": "dGXO010WBadQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4mtiOsqyyd9l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#7. Training your own Model"
      ]
    },
    {
      "metadata": {
        "id": "Wa6yD33dDusf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part of the tutorial shows how you can train your own sequence labeling and text\n",
        "classification models using state-of-the-art word embeddings.\n",
        "\n",
        "For this tutorial, we assume that you're familiar with the [base types](/resources/docs/TUTORIAL_1_BASICS.md) of this\n",
        "library and how [word embeddings](/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md) work (ideally, you also know how [flair embeddings](/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md) work). You should also know how to [load\n",
        "a corpus](/resources/docs/TUTORIAL_6_CORPUS.md).\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pK8FpjEZD1uL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a Sequence Labeling Model\n",
        "\n",
        "Here is example code for a small NER model trained over CoNLL-03 data, using simple GloVe embeddings. To run this code, you first need to obtain the CoNLL-03 English dataset (alternatively, use `NLPTaskDataFetcher.load_corpus(NLPTask.WNUT)` instead for a task with freely available data).\n",
        "\n",
        "In this example, we downsample the data to 10% of the original data:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UMzBr8FGD-Ar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import TaggedCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "from typing import List\n",
        "\n",
        "# 1. get the corpus\n",
        "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1)\n",
        "print(corpus)\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary.idx2item)\n",
        "\n",
        "# 4. initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "\n",
        "    WordEmbeddings('glove'),\n",
        "\n",
        "    # comment in this line to use character embeddings\n",
        "    # CharacterEmbeddings(),\n",
        "\n",
        "    # comment in these lines to use flair embeddings\n",
        "    # FlairEmbeddings('news-forward'),\n",
        "    # FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)\n",
        "\n",
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# 7. start training\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=150)\n",
        "\n",
        "# 8. plot training curves (optional)\n",
        "from flair.visual.training_curves import Plotter\n",
        "plotter = Plotter()\n",
        "plotter.plot_training_curves('resources/taggers/example-ner/loss.tsv')\n",
        "plotter.plot_weights('resources/taggers/example-ner/weights.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EEk7mJiyEFAx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Alternatively, try using a stacked embedding with FlairEmbeddings and GloVe, over the full data, for 150 epochs.\n",
        "This will give you the state-of-the-art accuracy we report in the paper. To see the full code to reproduce experiments,\n",
        "check [here](/resources/docs/EXPERIMENTS.md).\n",
        "\n",
        "Once the model is trained you can use it to predict the class of new sentences. Just call the `predict` method of the\n",
        "model.\n"
      ]
    },
    {
      "metadata": {
        "id": "BmJil2J-EKSU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the model you trained\n",
        "model = SequenceTagger.load_from_file('resources/taggers/example-ner/final-model.pt')\n",
        "\n",
        "# create example sentence\n",
        "sentence = Sentence('I love Berlin')\n",
        "\n",
        "# predict tags and print\n",
        "model.predict(sentence)\n",
        "\n",
        "print(sentence.to_tagged_string())\n",
        "#If the model works well, it will correctly tag 'Berlin' as a location in this example."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6caDiXkuEOfY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Training a Text Classification Model\n",
        "\n",
        "Here is example code for training a text classifier over the AGNews corpus, using  a combination of simple GloVe\n",
        "embeddings and Flair embeddings. You need to download the AGNews first to run this code. \n",
        "The AGNews corpus can be downloaded [here](https://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html).\n",
        "\n",
        "In this example, we downsample the data to 10% of the original data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "su4iApejEdOr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import TaggedCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "\n",
        "# 1. get the corpus\n",
        "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.AG_NEWS, 'path/to/data/folder').downsample(0.1)\n",
        "\n",
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()\n",
        "\n",
        "# 3. make a list of word embeddings\n",
        "word_embeddings = [WordEmbeddings('glove'),\n",
        "\n",
        "                   # comment in flair embeddings for state-of-the-art results \n",
        "                   # FlairEmbeddings('news-forward'),\n",
        "                   # FlairEmbeddings('news-backward'),\n",
        "                   ]\n",
        "\n",
        "# 4. init document embedding by passing list of word embeddings\n",
        "document_embeddings: DocumentLSTMEmbeddings = DocumentLSTMEmbeddings(word_embeddings,\n",
        "                                                                     hidden_size=512,\n",
        "                                                                     reproject_words=True,\n",
        "                                                                     reproject_words_dimension=256,\n",
        "                                                                     )\n",
        "\n",
        "# 5. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)\n",
        "\n",
        "# 6. initialize the text classifier trainer\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# 7. start the training\n",
        "trainer.train('resources/taggers/ag_news',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              anneal_factor=0.5,\n",
        "              patience=5,\n",
        "              max_epochs=150)\n",
        "\n",
        "# 8. plot training curves (optional)\n",
        "from flair.visual.training_curves import Plotter\n",
        "plotter = Plotter()\n",
        "plotter.plot_training_curves('resources/taggers/ag_news/loss.tsv')\n",
        "plotter.plot_weights('resources/taggers/ag_news/weights.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mYvCdGD7Ejqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Once the model is trained you can load it to predict the class of new sentences. Just call the `predict` method of the model.\n",
        "\n",
        "classifier = TextClassifier.load_from_file('resources/taggers/ag_news/final-model.pt')\n",
        "\n",
        "# create example sentence\n",
        "sentence = Sentence('France is the current world cup winner.')\n",
        "\n",
        "# predict tags and print\n",
        "classifier.predict(sentence)\n",
        "\n",
        "print(sentence.labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d59tnaEaEsTC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-Dataset Training\n",
        "\n",
        "Now, let us train a single model that can PoS tag text in both English and German. To do this, we load both the English and German UD corpora and create a MultiCorpus object. We also use the new multilingual Flair embeddings for this task. \n",
        "\n",
        "All the rest is same as before, e.g.: "
      ]
    },
    {
      "metadata": {
        "id": "MliaO3XWE10a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from flair.data import MultiCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings\n",
        "from flair.training_utils import EvaluationMetric\n",
        "\n",
        "\n",
        "# 1. get the corpora - English and German UD\n",
        "corpus: MultiCorpus = NLPTaskDataFetcher.load_corpora([NLPTask.UD_ENGLISH, NLPTask.UD_GERMAN]).downsample(0.1)\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'upos'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary.idx2item)\n",
        "\n",
        "# 4. initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "\n",
        "    # we use multilingual Flair embeddings in this task\n",
        "    FlairEmbeddings('multi-forward'),\n",
        "    FlairEmbeddings('multi-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)\n",
        "\n",
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# 7. start training\n",
        "trainer.train('resources/taggers/example-universal-pos',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=150,\n",
        "              evaluation_metric=EvaluationMetric.MICRO_ACCURACY)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QfRcpLbkE48-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that here we use the MICRO_ACCURACY evaluation metric instead of the default MICRO_F1_SCORE. This gives you a multilingual model. Try experimenting with more languages!"
      ]
    },
    {
      "metadata": {
        "id": "gd9LPGgCE62V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Plotting Training Curves and Weights\n",
        "\n",
        "Flair includes a helper method to plot training curves and weights in the neural network.\n",
        "The `ModelTrainer` automatically generates a `loss.tsv` and a `weights.txt` file in the result folder.\n",
        "\n",
        "After training, simple point the plotter to these files:"
      ]
    },
    {
      "metadata": {
        "id": "IbhqlU98E_AU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.visual.training_curves import Plotter\n",
        "plotter = Plotter()\n",
        "plotter.plot_training_curves('loss.tsv')\n",
        "plotter.plot_weights('weights.txt') #This generates PNG plots in the result folder."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKI7USf3FLLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Resuming Training\n",
        "\n",
        "If you want to stop the training at some point and resume it at a later point, you should train with the parameter\n",
        "`checkpoint` set to `True`.\n",
        "This will save the model plus training parameters after every epoch.\n",
        "Thus, you can load the model plus trainer at any later point and continue the training exactly there where you have\n",
        "left.\n",
        "\n",
        "The example code below shows how to train, stop, and continue training of a `SequenceTagger`.\n",
        "Same can be done for `TextClassifier`."
      ]
    },
    {
      "metadata": {
        "id": "sIO4-Y0pFURs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import TaggedCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "from typing import List\n",
        "\n",
        "# 1. get the corpus\n",
        "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1)\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "# 4. initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "    WordEmbeddings('glove')\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)\n",
        "\n",
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.training_utils import EvaluationMetric\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# 7. start training\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              EvaluationMetric.MICRO_F1_SCORE,\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=150,\n",
        "              checkpoint=True)\n",
        "\n",
        "# 8. stop training at any point\n",
        "\n",
        "# 9. continue trainer at later point\n",
        "from pathlib import Path\n",
        "\n",
        "trainer = ModelTrainer.load_from_checkpoint(Path('resources/taggers/example-ner/checkpoint.pt'), 'SequenceTagger', corpus)\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              EvaluationMetric.MICRO_F1_SCORE,\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=150,\n",
        "              checkpoint=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n68gQA5wBiNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scalability: Training on Large Data Sets\n",
        "\n",
        "The main thing to consider when using `FlairEmbeddings` (which you should) is that they are\n",
        "somewhat costly to generate for large training data sets. Depending on your setup, you can\n",
        "set options to optimize training time. There are three questions to ask:\n",
        "\n",
        "1. Do you have a GPU?\n",
        "\n",
        "`CharLMEmbeddings` are generated using Pytorch RNNs and are thus optimized for GPUs. If you have one,\n",
        "you can set large mini-batch sizes to make use of batching. If not, you may want to use smaller language models.\n",
        "For English, we package 'fast' variants of our embeddings, loadable like this: `FlairEmbeddings('news-forward-fast')`.\n",
        "\n",
        "2. Do embeddings for the entire dataset fit into memory?\n",
        "\n",
        "In the best-case scenario, all embeddings for the dataset fit into your regular memory, which greatly increases\n",
        "training speed. If this is not the case, you must set the flag `embeddings_in_memory=False` in the respective trainer\n",
        " (i.e. `ModelTrainer`) to\n",
        "avoid memory problems. With the flag, embeddings are either (a) recomputed at each epoch or (b)\n",
        "retrieved from disk if you choose to materialize to disk. \n",
        "\n",
        "3. Do you have a fast hard drive?\n",
        "\n",
        "If you have a fast hard drive, consider materializing the embeddings to disk. You can do this my instantiating FlairEmbeddings as follows: `FlairEmbeddings('news-forward-fast', use_cache=True)`. This can help if embeddings do not fit into memory. Also if you do not have a GPU and want to do repeat experiments on the same dataset, this helps because embeddings need only be computed once and will then always be retrieved from disk. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TEq2bUx5FaQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Next\n",
        "\n",
        "You can now either look into [optimizing your model](/resources/docs/TUTORIAL_8_MODEL_OPTIMIZATION.md) or\n",
        "[training your own embeddings](/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md).\n",
        "Drag and Drop\n",
        "The image will be downloaded by Fatkun\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RGCyj7DRyd7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#8. Optimizing our models"
      ]
    },
    {
      "metadata": {
        "id": "our0fQHOFz_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is part 8 of the tutorial, in which we look into how we can improve the quality of our model by selecting\n",
        "the right set of model and hyper parameters.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "10QNK5yLF0oh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Selecting Hyper Parameters\n",
        "\n",
        "Flair includes a wrapper for the well-known hyper parameter selection tool\n",
        "[hyperopt](https://github.com/hyperopt/hyperopt).\n",
        "\n",
        "First you need to load your corpus. If you want to load the [AGNews corpus](https://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\n",
        "used in the following example, you first need to download it and convert it into the correct format. Please\n",
        "check [tutorial 6](/resources/docs/TUTORIAL_6_CORPUS.md) for more details.\n"
      ]
    },
    {
      "metadata": {
        "id": "aZBD_v_JF5T4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import TaggedCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "\n",
        "# load your corpus\n",
        "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.AG_NEWS, base_path='/resources/tasks/ag_news')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8ieX-sJGBHL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Second you need to define the search space of parameters.\n",
        "Therefore, you can use all\n",
        "[parameter expressions](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions) defined by hyperopt.\n"
      ]
    },
    {
      "metadata": {
        "id": "BznuGqKgF9Nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from hyperopt import hp\n",
        "from flair.hyperparameter.param_selection import SearchSpace, Parameter\n",
        "\n",
        "# define your search space\n",
        "search_space = SearchSpace()\n",
        "search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[\n",
        "    [ WordEmbeddings('en') ], \n",
        "    [ CharLMEmbeddings('news-forward'), CharLMEmbeddings('news-backward') ]\n",
        "])\n",
        "search_space.add(Parameter.HIDDEN_SIZE, hp.choice, options=[32, 64, 128])\n",
        "search_space.add(Parameter.RNN_LAYERS, hp.choice, options=[1, 2])\n",
        "search_space.add(Parameter.DROPOUT, hp.uniform, low=0.0, high=0.5)\n",
        "search_space.add(Parameter.LEARNING_RATE, hp.choice, options=[0.05, 0.1, 0.15, 0.2])\n",
        "search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[8, 16, 32])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxlClGPcGGqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Attention: You should always add your embeddings to the search space (as shown above). If you don't want to test\n",
        "different kind of embeddings, simply pass just one embedding option to the search space, which will than be used in\n",
        "every test run. Here is an example:"
      ]
    },
    {
      "metadata": {
        "id": "HSmAAop-GLD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[\n",
        "    [ CharLMEmbeddings('news-forward'), CharLMEmbeddings('news-backward') ]\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A3lRzLctGOfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the last step you have to create the actual parameter selector. \n",
        "Depending on the task you need either to define a `TextClassifierParamSelector` or a `SequenceTaggerParamSelector` and \n",
        "start the optimization.\n",
        "You can define the maximum number of evaluation runs hyperopt should perform (`max_evals`).\n",
        "A evaluation run performs the specified number of epochs (`max_epochs`). \n",
        "To overcome the issue of noisy evaluation scores, we take the average over the last three evaluation scores (either\n",
        "`dev_score` or `dev_loss`) from the evaluation run, which represents the final score and will be passed to hyperopt.\n",
        "Additionally, you can specify the number of runs per evaluation run (`training_runs`). \n",
        "If you specify more than one training run, one evaluation run will be executed the specified number of times.\n",
        "The final evaluation score will be the average over all those runs."
      ]
    },
    {
      "metadata": {
        "id": "GEzeaxkZGSTe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.hyperparameter.param_selection import TextClassifierParamSelector, OptimizationValue\n",
        "\n",
        "# create the parameter selector\n",
        "param_selector = TextClassifierParamSelector(\n",
        "    corpus, \n",
        "    False, \n",
        "    'resources/results', \n",
        "    'lstm',\n",
        "    max_epochs=50, \n",
        "    training_runs=3,\n",
        "    optimization_value=OptimizationValue.DEV_SCORE\n",
        ")\n",
        "\n",
        "# start the optimization\n",
        "param_selector.optimize(search_space, max_evals=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "udxpPXJlGXU3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The parameter settings and the evaluation scores will be written to `param_selection.txt` in the result directory.\n",
        "While selecting the best parameter combination we do not store any model to disk. We also do not perform a test run\n",
        "during training, we just evaluate the model once after training on the test set for logging purpose."
      ]
    },
    {
      "metadata": {
        "id": "3bsKsq1qGbHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Finding the best Learning Rate\n",
        "\n",
        "The learning rate is one of the most important hyper parameter and it fundamentally depends on the topology of the loss\n",
        "landscape via the architecture of your model and the training data it consumes. An optimal learning will improve your\n",
        "training speed and hopefully give more performant models. A simple technique described by Leslie Smith's\n",
        "[Cyclical Learning Rates for Training](https://arxiv.org/abs/1506.01186) paper is to train your model starting with a\n",
        "very low learning rate and increases the learning rate exponentially at every batch update of SGD. By plotting the loss\n",
        "with respect to the learning rate we will typically observe three distinct phases: for low learning rates the loss does\n",
        "not improve, an optimal learning rate range where the loss drops the steepest and the final phase where the loss\n",
        "explodes as the learning rate becomes too big. With such a plot, the optimal learning rate selection is as easy as\n",
        "picking the highest one from the optimal phase.\n",
        "\n",
        "In order to run such an experiment start with your initialized `ModelTrainer` and call `find_learning_rate()` with the\n",
        "`base_path` and the file name in which to records the learning rates and losses. Then plot the generated results via the\n",
        "`Plotter`'s `plot_learning_rate()` function and have a look at the `learning_rate.png` image to select the optimal\n",
        "learning rate:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D9QYgFWUGh8D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.data import TaggedCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "from flair.trainers import ModelTrainer\n",
        "from typing import List\n",
        "\n",
        "# 1. get the corpus\n",
        "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1)\n",
        "print(corpus)\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary.idx2item)\n",
        "\n",
        "# 4. initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "    WordEmbeddings('glove'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)\n",
        "\n",
        "# 6. initialize trainer\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# 7. find learning rate\n",
        "learning_rate_tsv = ModelTrainer.find_learning_rate('resources/taggers/example-ner',\n",
        "                                                    'learning_rate.tsv')\n",
        "\n",
        "# 8. plot the learning rate finder curve\n",
        "plotter = Plotter()\n",
        "plotter.plot_learning_rate(learning_rate_tsv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nnRmSkKjG2og",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom Optimizers\n",
        "\n",
        "You can now use any of PyTorch's optimizers for training when initializing a `ModelTrainer`. To give the optimizer any extra options just specify it as shown with the `weight_decay` example:\n"
      ]
    },
    {
      "metadata": {
        "id": "CavBzN9aGoi7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus,\n",
        "                                     optimizer=Adam, weight_decay=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FjKLBNL2FjkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### AdamW and SGDW\n",
        "\n",
        "Weight decay is typically used by optimization methods to reduce over-fitting and it essentially adds a weight\n",
        "regularizer to the loss function via the `weight_decay` parameter of the optimizer. The way it is implemented in PyTorch\n",
        "this factor is confounded with the `learning_rate` and is essentially implementing L2 regularization. In the paper from\n",
        "Ilya Loshchilov and Frank Hutter [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101) the\n",
        "authors suggest to actually do weight decay rather than L2 regularization and they call their method AdamW and SGDW for\n",
        "the corresponding Adam and SGD versions. Empirically the results via these optimizers are better than their\n",
        "corresponding L2 regularized versions. However as the learning rate and weight decay are decoupled in these methods,\n",
        "any learning rate scheduling has to change both these terms. Not to worry, we automatically switch \n",
        "schedulers that do this when these optimizers are used.\n",
        "\n",
        "To use these optimizers just create the `ModelTrainer` with `AdamW` or `SGDW` together with any extra options as shown:"
      ]
    },
    {
      "metadata": {
        "id": "d_pgSMwLHBqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from flair.optim import SGDW\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus,\n",
        "                                     optimizer=SGDW, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkKvqT8bHGUW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Next\n",
        "\n",
        "The last tutorial is about [training your own embeddings](/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md).\n",
        "Drag and Drop\n",
        "The image will be downloaded by Fatkun\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Rj2fT7amzRQV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#9. Training your own Flair Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "38zovM4HHOG6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Flair Embeddings are the secret sauce in Flair, allowing us to achieve state-of-the-art accuracies across a\n",
        "range of NLP tasks.\n",
        "This tutorial shows you how to train your own Flair embeddings, which may come in handy if you want to apply Flair\n",
        "to new languages or domains."
      ]
    },
    {
      "metadata": {
        "id": "evHcXeJrHQHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing a Text Corpus\n",
        "\n",
        "Language models are trained with plain text. In the case of character LMs, we train them to predict the next character in a sequence of characters.\n",
        "\n",
        "To train your own model, you first need to identify a suitably large corpus. In our experiments, we used corpora that have about 1 billion words.\n",
        "\n",
        "You need to split your corpus into train, validation and test portions.\n",
        "Our trainer class assumes that there is a folder for the corpus in which there is a 'test.txt' and a 'valid.txt' with test and validation data.\n",
        "\n",
        "Importantly, there is also a folder called 'train' that contains the training data in splits.\n",
        "For instance, the billion word corpus is split into 100 parts.\n",
        "\n",
        "The splits are necessary if all the data does not fit into memory, in which case the trainer randomly iterates through all splits."
      ]
    },
    {
      "metadata": {
        "id": "8t4P0v_RHkt3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "So, the folder structure must look like this:\n",
        "\n",
        "```\n",
        "corpus/\n",
        "corpus/train/\n",
        "corpus/train/train_split_1\n",
        "corpus/train/train_split_2\n",
        "corpus/train/...\n",
        "corpus/train/train_split_X\n",
        "corpus/test.txt\n",
        "corpus/valid.txt\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yT2UYAfSHmNZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the Language Model\n",
        "\n",
        "Once you have this folder structure, simply point the `LanguageModelTrainer` class to it to start learning a model."
      ]
    },
    {
      "metadata": {
        "id": "SrwW0__LHq6i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from flair.data import Dictionary\n",
        "from flair.models import LanguageModel\n",
        "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
        "\n",
        "# are you training a forward or backward LM?\n",
        "is_forward_lm = True\n",
        "\n",
        "# load the default character dictionary\n",
        "dictionary: Dictionary = Dictionary.load('chars')\n",
        "\n",
        "# get your corpus, process forward and at the character level\n",
        "corpus = TextCorpus(Path('/path/to/your/corpus'),\n",
        "                    dictionary,\n",
        "                    is_forward_lm,\n",
        "                    character_level=True)\n",
        "\n",
        "# instantiate your language model, set hidden size and number of layers\n",
        "language_model = LanguageModel(dictionary,\n",
        "                               is_forward_lm,\n",
        "                               hidden_size=128,\n",
        "                               nlayers=1)\n",
        "\n",
        "# train your language model\n",
        "trainer = LanguageModelTrainer(language_model, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/language_model',\n",
        "              sequence_length=10,\n",
        "              mini_batch_size=10,\n",
        "              max_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQJ2-rFDHySw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The parameters in this script are very small. We got good results with a hidden size of 1024 or 2048, a sequence length\n",
        "of 250, and a mini-batch size of 100.\n",
        "Depending on your resources, you can try training large models, but beware that you need a very powerful GPU and a lot\n",
        "of time to train a model (we train for > 1 week)."
      ]
    },
    {
      "metadata": {
        "id": "LP6REJjAH2Xv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using the LM as Embeddings\n",
        "\n",
        "Once you have the LM trained, using it as embeddings is easy. Just load the model into the `CharLMEmbeddings` class and\n",
        "use as you would any other embedding in Flair:"
      ]
    },
    {
      "metadata": {
        "id": "OkqEhkDXH53l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = Sentence('I love Berlin')\n",
        "\n",
        "# init embeddings from your trained LM\n",
        "char_lm_embeddings = FlairEmbeddings('resources/taggers/language_model/best-lm.pt')\n",
        "\n",
        "# embed sentence\n",
        "char_lm_embeddings.embed(sentence)\n",
        "\n",
        "#Done!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4tWWhKsIF9x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-Latin Alphabets\n",
        "\n",
        "If you train embeddings for a language that uses a non-latin alphabet such as Arabic or Japanese, you need to create your own character dictionary first. You can do this with the following code snippet: "
      ]
    },
    {
      "metadata": {
        "id": "aYWETXa_ILtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make an empty character dictionary\n",
        "from flair.data import Dictionary\n",
        "char_dictionary: Dictionary = Dictionary()\n",
        "\n",
        "# counter object\n",
        "import collections\n",
        "counter = collections.Counter()\n",
        "\n",
        "processed = 0\n",
        "\n",
        "import glob\n",
        "files = glob.glob('/path/to/your/corpus/files/*.*')\n",
        "\n",
        "print(files)\n",
        "for file in files:\n",
        "    print(file)\n",
        "\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        tokens = 0\n",
        "        for line in f:\n",
        "\n",
        "            processed += 1            \n",
        "            chars = list(line)\n",
        "            tokens += len(chars)\n",
        "\n",
        "            # Add chars to the dictionary\n",
        "            counter.update(chars)\n",
        "\n",
        "            # comment this line in to speed things up (if the corpus is too large)\n",
        "            # if tokens > 50000000: break\n",
        "\n",
        "    # break\n",
        "\n",
        "total_count = 0\n",
        "for letter, count in counter.most_common():\n",
        "    total_count += count\n",
        "\n",
        "print(total_count)\n",
        "print(processed)\n",
        "\n",
        "sum = 0\n",
        "idx = 0\n",
        "for letter, count in counter.most_common():\n",
        "    sum += count\n",
        "    percentile = (sum / total_count)\n",
        "\n",
        "    # comment this line in to use only top X percentile of chars, otherwise filter later\n",
        "    # if percentile < 0.00001: break\n",
        "\n",
        "    char_dictionary.add_item(letter)\n",
        "    idx += 1\n",
        "    print('%d\\t%s\\t%7d\\t%7d\\t%f' % (idx, letter, count, sum, percentile))\n",
        "\n",
        "print(char_dictionary.item2idx)\n",
        "\n",
        "import pickle\n",
        "with open('/path/to/your_char_mappings', 'wb') as f:\n",
        "    mappings = {\n",
        "        'idx2item': char_dictionary.idx2item,\n",
        "        'item2idx': char_dictionary.item2idx\n",
        "    }\n",
        "    pickle.dump(mappings, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JcC1_kQEIRiD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#You can then use this dictionary instead of the default one in your code for training the language model: \n",
        "import pickle\n",
        "dictionary = Dictionary.load_from_file('/path/to/your_char_mappings')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kY-h73PRIXju",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameters\n",
        "\n",
        "You might to play around with some of the learning parameters in the `LanguageModelTrainer`.\n",
        "For instance, we generally find that an initial learning rate of 20, and an annealing factor of 4 is pretty good for most corpora.\n",
        "\n",
        "You might also want to modify the 'patience' value of the learning rate scheduler. We currently have it at 25, meaning that if the training loss does not improve for 25 splits, it decreases the learning rate."
      ]
    },
    {
      "metadata": {
        "id": "5xznjbHnFp-B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Consider Contributing your LM\n",
        "\n",
        "If you train a good LM for a language or domain we don't yet have in Flair, consider contacting us! \n",
        "We would be happy to integrate more LMs into the library so that other people can use them!\n",
        "\n",
        "Drag and Drop\n",
        "The image will be downloaded by Fatkun\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ei4eRodRIrt-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## End of Flair Tutorials"
      ]
    },
    {
      "metadata": {
        "id": "A6yRoTpnzX1I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}